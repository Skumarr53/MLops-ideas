{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bfe71a8-1178-43be-97b9-c2d014e5b35d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install loguru==0.7.2\n",
    "%pip install hydra-core==1.3\n",
    "%pip install python-dotenv==1.0.1\n",
    "%pip install numpy==1.24.4\n",
    "%pip install cryptography==43.0.1\n",
    "%pip install gensim==4.3.3\n",
    "%pip install cython==3.0.11\n",
    "%pip install spacy==3.4.4 #3.0.4\n",
    "%pip install thinc==8.1.7\n",
    "%pip install pandas==2.0.0\n",
    "%pip install snowflake-connector-python==3.12.2\n",
    "%pip install transformers==4.46.1\n",
    "%pip install pyarrow==16.0.0\n",
    "%pip install datasets==3.1.0\n",
    "%pip install evaluate==0.4.3\n",
    "%pip install pyspark==3.5.3\n",
    "%pip install dask==2023.10.1 \n",
    "%pip install distributed==2023.10.1\n",
    "%pip install torch==2.0.0\n",
    "# %pip install cymem==2.0.8\n",
    "# %pip install scikit-learn==1.1.0\n",
    "# %pip install typer==0.7.0\n",
    "# %pip install accelerate==0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343f743c-f18e-45f3-bf7a-b5e6ee9018bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 14:59:39\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 14:59:39\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n"
     ]
    }
   ],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d2058ae-b108-4d31-ad2a-d66d9f78428c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import time \n",
    "\n",
    "# while 1>0:\n",
    "#   time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34e64e8-d161-48f1-969d-61fcb23f25e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/utils/config.py:75: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  with hydra.initialize(config_path=\"../configs\"):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 14:59:57\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mlogging_setup.py:62\u001B[0m | Logging setup completed.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n  warn(f\"Failed to load image Python extension: {e}\")\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/topic_modelling_package/utils/config.py:15: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  with hydra.initialize(config_path=\"../config\"):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:00:11\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mlogging.py:45\u001B[0m | Logging setup completed.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.dataframe as dd\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from centralized_nlp_package.data_access import (\n",
    "    read_from_snowflake,\n",
    "    write_dataframe_to_snowflake\n",
    ")\n",
    "from centralized_nlp_package.data_processing import (\n",
    "  check_pd_dataframe_for_records,\n",
    "    initialize_dask_client,\n",
    "    df_apply_transformations,\n",
    "    dask_compute_with_progress,\n",
    "    pandas_to_spark,\n",
    "    convert_columns_to_timestamp\n",
    ")\n",
    "from centralized_nlp_package.text_processing import (initialize_spacy, get_match_set)\n",
    "\n",
    "from topic_modelling_package.reports import create_topic_dict, generate_topic_report, replace_separator_in_dict_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ae8c7d-fd45-4fe2-996e-26213bd40408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216825c8-8e97-4fcd-9d0f-8fad574db1a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tsQuery= (\"SELECT CALL_ID,ENTITY_ID, DATE, FILT_MD, FILT_QA, SENT_LABELS_FILT_MD,\"\n",
    "         \"SENT_LABELS_FILT_QA, CALL_NAME,COMPANY_NAME,EARNINGS_CALL,ERROR,TRANSCRIPT_STATUS,UPLOAD_DT_UTC,VERSION_ID,EVENT_DATETIME_UTC,PARSED_DATETIME_EASTERN_TZ \"\n",
    "          \"FROM EDS_PROD.QUANT_LIVE.CTS_FUND_COMBINED_SCORES_H  t2  ORDER BY PARSED_DATETIME_EASTERN_TZ DESC LIMIT 10\")\n",
    "          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd0d94f-0cdc-4e11-a625-18eacab0caf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:00:13\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36msnowflake_utils.py:226\u001B[0m | Reading data from Snowflake using Spark.\nSpark session already initialized; reusing existing session.\nSpark session already initialized; reusing existing session.\n\u001B[32m2024-12-13 15:00:13\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mhelper.py:27\u001B[0m | Environment provided by user: quant\n\u001B[32m2024-12-13 15:00:14\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36msnowflake_utils.py:123\u001B[0m | Retrieved secrets from AKV successfully.\n\u001B[32m2024-12-13 15:00:14\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36msnowflake_utils.py:135\u001B[0m | Private key loaded successfully.\n\u001B[32m2024-12-13 15:00:14\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36msnowflake_utils.py:147\u001B[0m | Private key serialized to PEM format.\n\u001B[32m2024-12-13 15:00:14\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36msnowflake_utils.py:155\u001B[0m | Private key decoded and cleaned.\n\u001B[32m2024-12-13 15:00:14\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36msnowflake_utils.py:190\u001B[0m | Snowflake connection options constructed.\n\u001B[32m2024-12-13 15:00:14\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36msnowflake_utils.py:231\u001B[0m | Executing query: SELECT CALL_ID,ENTITY_ID, DATE, FILT_MD, FILT_QA, SENT_LABELS_FILT_MD,SENT_LABELS_FILT_QA, CALL_NAME,COMPANY_NAME,EARNINGS_CALL,ERROR,TRANSCRIPT_STATUS,UPLOAD_DT_UTC,VERSION_ID,EVENT_DATETIME_UTC,PARSED_DATETIME_EASTERN_TZ FROM EDS_PROD.QUANT_LIVE.CTS_FUND_COMBINED_SCORES_H  t2  ORDER BY PARSED_DATETIME_EASTERN_TZ DESC LIMIT 10\n\u001B[32m2024-12-13 15:00:15\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36msnowflake_utils.py:236\u001B[0m | Query executed successfully and Spark DataFrame created.\n"
     ]
    }
   ],
   "source": [
    "resultspkdf = read_from_snowflake(tsQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab39203c-2b2a-4e5d-bb24-ea64de45862b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [CALL_ID, EARNINGS_CALL] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "currdf_old = resultspkdf.toPandas()\n",
    "currdf_ref = currdf_old.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf1c690-35e2-4103-bded-a7d480307801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Checking records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b0425b-05b5-4ee3-9626-24199e4e23b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70fdf110-b3db-4360-b43d-57f303b7ee13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data spans from 2024-12-13 01:14:51 to 2024-12-13 01:14:51and has 10 rows and 16 columns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if len(currdf_old)>0:\n",
    "    print('The data spans from ' + str(currdf_old['PARSED_DATETIME_EASTERN_TZ'].min()) + ' to ' + str(currdf_old['PARSED_DATETIME_EASTERN_TZ'].max()) + 'and has ' + str(currdf_old.shape[0]) + ' rows and ' + str(currdf_old.shape[1]) + ' columns.')\n",
    "else:\n",
    "    print('No new transcripts to parse.')\n",
    "    dbutils.notebook.exit(1)\n",
    "    os._exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f58535-e5d2-4466-9c00-d98556ed0f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Reafctored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1061f39-2a0e-4888-aeee-5efab9100d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:230\u001B[0m | The data spans from 2024-12-13 01:14:51 to 2024-12-13 01:14:51 and has 10 rows and 16 columns.\n"
     ]
    }
   ],
   "source": [
    "check_pd_dataframe_for_records(currdf_old, datetime_col = 'PARSED_DATETIME_EASTERN_TZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72fabfe9-aa09-4c42-88bc-8edc69153769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Apply Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2628550-3adb-4da2-9970-5e7e37839ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8743b48d-fe22-4281-b2a2-ce0c3cc836e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code processes a DataFrame named currdf_old by converting the CALL_ID column to strings and parsing string representations of lists in several columns into actual lists using ast.literal_eval. It calculates the lengths of the lists in the FILT_MD and FILT_QA columns, storing these lengths in new columns LEN_FILT_MD and LEN_FILT_QA. The code then filters out sentences in the FILT_QA column that end with a question mark, updating both the FILT_QA and SENT_LABELS_FILT_QA columns accordingly. This transformation is part of a workflow to prepare and validate data by comparing outputs from the original and refactored code implementations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6485459a-2ee9-4de2-aab5-632597626fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CALL_ID</th>\n",
       "      <th>ENTITY_ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>FILT_MD</th>\n",
       "      <th>FILT_QA</th>\n",
       "      <th>SENT_LABELS_FILT_MD</th>\n",
       "      <th>SENT_LABELS_FILT_QA</th>\n",
       "      <th>CALL_NAME</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "      <th>EARNINGS_CALL</th>\n",
       "      <th>ERROR</th>\n",
       "      <th>TRANSCRIPT_STATUS</th>\n",
       "      <th>UPLOAD_DT_UTC</th>\n",
       "      <th>VERSION_ID</th>\n",
       "      <th>EVENT_DATETIME_UTC</th>\n",
       "      <th>PARSED_DATETIME_EASTERN_TZ</th>\n",
       "      <th>LEN_FILT_MD</th>\n",
       "      <th>LEN_FILT_QA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3133014</td>\n",
       "      <td>000CCP-E</td>\n",
       "      <td>2024-12-12</td>\n",
       "      <td>[Let me start by addressing our third quarter ...</td>\n",
       "      <td>[Sorry if I missed this in the earlier comment...</td>\n",
       "      <td>[0, -1, -1, 1, -1, -1, 1, 1, 1, 0, 1, -1, 0, 0...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0...</td>\n",
       "      <td>Q3 2025 Earnings Call</td>\n",
       "      <td>AstroNova, Inc.</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>CorrectedTranscript</td>\n",
       "      <td>2024-12-12 16:55:56</td>\n",
       "      <td>7661968</td>\n",
       "      <td>2024-12-12 14:00:00</td>\n",
       "      <td>2024-12-13 01:14:51</td>\n",
       "      <td>96</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3145024</td>\n",
       "      <td>05HFQP-E</td>\n",
       "      <td>2024-12-11</td>\n",
       "      <td>[It's my pleasure to join you today., In the i...</td>\n",
       "      <td>[ for taking my question., I just wondered if ...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>Q3 2024 Earnings Call</td>\n",
       "      <td>Industria de Diseño Textil SA</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>CorrectedTranscript</td>\n",
       "      <td>2024-12-12 20:29:24</td>\n",
       "      <td>7662284</td>\n",
       "      <td>2024-12-11 00:00:00</td>\n",
       "      <td>2024-12-13 01:14:51</td>\n",
       "      <td>100</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3142307</td>\n",
       "      <td>0GYVRX-E</td>\n",
       "      <td>2024-12-12</td>\n",
       "      <td>[I'll start by sharing a high-level overview o...</td>\n",
       "      <td>[First, I just wanted to ask about your Q4 out...</td>\n",
       "      <td>[0, 0, 0, 0, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>Q3 2025 Earnings Call</td>\n",
       "      <td>The Lovesac Co.</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>CorrectedTranscript</td>\n",
       "      <td>2024-12-12 17:58:40</td>\n",
       "      <td>7662053</td>\n",
       "      <td>2024-12-12 13:30:00</td>\n",
       "      <td>2024-12-13 01:14:51</td>\n",
       "      <td>164</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3130706</td>\n",
       "      <td>000RCZ-E</td>\n",
       "      <td>2024-12-12</td>\n",
       "      <td>[ for joining Nordson's fiscal 2024 fourth qua...</td>\n",
       "      <td>[Appreciate all the thoughts at the end there ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 1, 0,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, ...</td>\n",
       "      <td>Q4 2024 Earnings Call</td>\n",
       "      <td>Nordson Corp.</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>CorrectedTranscript</td>\n",
       "      <td>2024-12-12 18:35:23</td>\n",
       "      <td>7662109</td>\n",
       "      <td>2024-12-12 13:30:00</td>\n",
       "      <td>2024-12-13 01:14:51</td>\n",
       "      <td>140</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3138897</td>\n",
       "      <td>003NFW-E</td>\n",
       "      <td>2024-12-12</td>\n",
       "      <td>[We posted solid results in Q4 and I thank our...</td>\n",
       "      <td>[We're seeing a lot of consolidation activity,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0...</td>\n",
       "      <td>Q4 2024 Earnings Call</td>\n",
       "      <td>TC Transcontinental</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>CorrectedTranscript</td>\n",
       "      <td>2024-12-12 23:26:10</td>\n",
       "      <td>7662405</td>\n",
       "      <td>2024-12-12 13:00:00</td>\n",
       "      <td>2024-12-13 01:14:51</td>\n",
       "      <td>95</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CALL_ID ENTITY_ID  ... LEN_FILT_MD LEN_FILT_QA\n",
       "0  3133014  000CCP-E  ...          96          40\n",
       "1  3145024  05HFQP-E  ...         100          86\n",
       "2  3142307  0GYVRX-E  ...         164         205\n",
       "3  3130706  000RCZ-E  ...         140         248\n",
       "4  3138897  003NFW-E  ...          95         153\n",
       "\n",
       "[5 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "currdf_old['CALL_ID'] = currdf_old['CALL_ID'].apply(lambda x: str(x))\n",
    "currdf_old['FILT_MD'] = currdf_old['FILT_MD'].apply(ast.literal_eval)\n",
    "currdf_old['FILT_QA'] = currdf_old['FILT_QA'].apply(ast.literal_eval)\n",
    "currdf_old['SENT_LABELS_FILT_MD'] = currdf_old['SENT_LABELS_FILT_MD'].apply(ast.literal_eval)\n",
    "currdf_old['SENT_LABELS_FILT_QA'] = currdf_old['SENT_LABELS_FILT_QA'].apply(ast.literal_eval)\n",
    "currdf_old['LEN_FILT_MD'] = currdf_old['FILT_MD'].apply(len)\n",
    "currdf_old['LEN_FILT_QA'] = currdf_old['FILT_QA'].apply(len)\n",
    "currdf_old['SENT_LABELS_FILT_QA'] = currdf_old.apply(lambda x: [x['SENT_LABELS_FILT_QA'][i] for i, sent in enumerate(x['FILT_QA']) if not sent.endswith('?')], axis=1)\n",
    "currdf_old['FILT_QA'] = currdf_old['FILT_QA'].apply(lambda x: [sent for sent in x if not sent.endswith('?')])\n",
    "\n",
    "\n",
    "currdf_old.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9474194e-099a-498a-bf9f-0a7e66768406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dadf0fb1-6c69-4b94-9d0d-28de0bfdcfa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'CALL_ID' to create 'CALL_ID'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'CALL_ID'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'FILT_MD' to create 'FILT_MD'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'FILT_MD'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'FILT_QA' to create 'FILT_QA'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'FILT_QA'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'SENT_LABELS_FILT_QA' to create 'SENT_LABELS_FILT_QA'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SENT_LABELS_FILT_QA'.\nThe DataFrames are not identical.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'SENT_LABELS_FILT_MD' to create 'SENT_LABELS_FILT_MD'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SENT_LABELS_FILT_MD'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'FILT_MD' to create 'LEN_FILT_MD'.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CALL_ID</th>\n",
       "      <th>ENTITY_ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>FILT_MD</th>\n",
       "      <th>FILT_QA</th>\n",
       "      <th>SENT_LABELS_FILT_MD</th>\n",
       "      <th>SENT_LABELS_FILT_QA</th>\n",
       "      <th>CALL_NAME</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "      <th>EARNINGS_CALL</th>\n",
       "      <th>ERROR</th>\n",
       "      <th>TRANSCRIPT_STATUS</th>\n",
       "      <th>UPLOAD_DT_UTC</th>\n",
       "      <th>VERSION_ID</th>\n",
       "      <th>EVENT_DATETIME_UTC</th>\n",
       "      <th>PARSED_DATETIME_EASTERN_TZ</th>\n",
       "      <th>LEN_FILT_MD</th>\n",
       "      <th>LEN_FILT_QA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3133014</td>\n",
       "      <td>000CCP-E</td>\n",
       "      <td>2024-12-12</td>\n",
       "      <td>[Let me start by addressing our third quarter ...</td>\n",
       "      <td>[Sorry if I missed this in the earlier comment...</td>\n",
       "      <td>[0, -1, -1, 1, -1, -1, 1, 1, 1, 0, 1, -1, 0, 0...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0...</td>\n",
       "      <td>Q3 2025 Earnings Call</td>\n",
       "      <td>AstroNova, Inc.</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>CorrectedTranscript</td>\n",
       "      <td>2024-12-12 16:55:56</td>\n",
       "      <td>7661968</td>\n",
       "      <td>2024-12-12 14:00:00</td>\n",
       "      <td>2024-12-13 01:14:51</td>\n",
       "      <td>96</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3145024</td>\n",
       "      <td>05HFQP-E</td>\n",
       "      <td>2024-12-11</td>\n",
       "      <td>[It's my pleasure to join you today., In the i...</td>\n",
       "      <td>[ for taking my question., I just wondered if ...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>Q3 2024 Earnings Call</td>\n",
       "      <td>Industria de Diseño Textil SA</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>CorrectedTranscript</td>\n",
       "      <td>2024-12-12 20:29:24</td>\n",
       "      <td>7662284</td>\n",
       "      <td>2024-12-11 00:00:00</td>\n",
       "      <td>2024-12-13 01:14:51</td>\n",
       "      <td>100</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3142307</td>\n",
       "      <td>0GYVRX-E</td>\n",
       "      <td>2024-12-12</td>\n",
       "      <td>[I'll start by sharing a high-level overview o...</td>\n",
       "      <td>[First, I just wanted to ask about your Q4 out...</td>\n",
       "      <td>[0, 0, 0, 0, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>Q3 2025 Earnings Call</td>\n",
       "      <td>The Lovesac Co.</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>CorrectedTranscript</td>\n",
       "      <td>2024-12-12 17:58:40</td>\n",
       "      <td>7662053</td>\n",
       "      <td>2024-12-12 13:30:00</td>\n",
       "      <td>2024-12-13 01:14:51</td>\n",
       "      <td>164</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3130706</td>\n",
       "      <td>000RCZ-E</td>\n",
       "      <td>2024-12-12</td>\n",
       "      <td>[ for joining Nordson's fiscal 2024 fourth qua...</td>\n",
       "      <td>[Appreciate all the thoughts at the end there ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 1, 0,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, ...</td>\n",
       "      <td>Q4 2024 Earnings Call</td>\n",
       "      <td>Nordson Corp.</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>CorrectedTranscript</td>\n",
       "      <td>2024-12-12 18:35:23</td>\n",
       "      <td>7662109</td>\n",
       "      <td>2024-12-12 13:30:00</td>\n",
       "      <td>2024-12-13 01:14:51</td>\n",
       "      <td>140</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3138897</td>\n",
       "      <td>003NFW-E</td>\n",
       "      <td>2024-12-12</td>\n",
       "      <td>[We posted solid results in Q4 and I thank our...</td>\n",
       "      <td>[We're seeing a lot of consolidation activity,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0...</td>\n",
       "      <td>Q4 2024 Earnings Call</td>\n",
       "      <td>TC Transcontinental</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>CorrectedTranscript</td>\n",
       "      <td>2024-12-12 23:26:10</td>\n",
       "      <td>7662405</td>\n",
       "      <td>2024-12-12 13:00:00</td>\n",
       "      <td>2024-12-13 01:14:51</td>\n",
       "      <td>95</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CALL_ID ENTITY_ID  ... LEN_FILT_MD LEN_FILT_QA\n",
       "0  3133014  000CCP-E  ...          96          40\n",
       "1  3145024  05HFQP-E  ...         100          86\n",
       "2  3142307  0GYVRX-E  ...         164         205\n",
       "3  3130706  000RCZ-E  ...         140         248\n",
       "4  3138897  003NFW-E  ...          95         153\n",
       "\n",
       "[5 rows x 18 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LEN_FILT_MD'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'FILT_QA' to create 'LEN_FILT_QA'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LEN_FILT_QA'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['SENT_LABELS_FILT_QA', 'FILT_QA'] to create 'SENT_LABELS_FILT_QA'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SENT_LABELS_FILT_QA'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'FILT_QA' to create 'FILT_QA'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'FILT_QA'.\n\u001B[32m2024-12-13 15:00:57\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n"
     ]
    }
   ],
   "source": [
    "## cleaner, easliy readble and understandable\n",
    "\n",
    "col_inti_tranform = [\n",
    "  (\"CALL_ID\",\"CALL_ID\", str),\n",
    "    (\"FILT_MD\", \"FILT_MD\", ast.literal_eval),\n",
    "    (\"FILT_QA\", \"FILT_QA\", ast.literal_eval),\n",
    "    (\"SENT_LABELS_FILT_QA\", \"SENT_LABELS_FILT_QA\", ast.literal_eval),\n",
    "    (\"SENT_LABELS_FILT_MD\", \"SENT_LABELS_FILT_MD\", ast.literal_eval),\n",
    "    ('LEN_FILT_MD', 'FILT_MD', len),\n",
    "    ('LEN_FILT_QA', 'FILT_QA', len),\n",
    "    ('SENT_LABELS_FILT_QA', ['SENT_LABELS_FILT_QA','FILT_QA'], (lambda x: [x['SENT_LABELS_FILT_QA'][i] \n",
    "                                                                           for i, sent in enumerate(x['FILT_QA']) \n",
    "                                                                           if not sent.endswith('?')])),\n",
    "    ('FILT_QA', 'FILT_QA', lambda x: [sent for sent in x if not sent.endswith('?')])\n",
    "]\n",
    "\n",
    "currdf_ref = df_apply_transformations(currdf_ref, col_inti_tranform)\n",
    "print(\"The DataFrames are not identical.\")\n",
    "currdf_ref.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6f06d0-a3ff-4276-8f74-54df47c3e32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert currdf_old.equals(currdf_ref), \"The DataFrames are not identical.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485847ba-9905-4173-9d97-5da86f9ea51c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spacy model initalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7ad2cd6-ba7f-4fb6-9f26-af21388d8359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Intialize spacy object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "836ae8d7-b52d-45d3-a870-22b86d9a2845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### old "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d83eff-3f95-48a6-b239-9c630468ef29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz (12.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 41.4 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting spacy<3.6.0,>=3.5.0\n  Downloading spacy-3.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 14.4 MB/s eta 0:00:00\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.9)\nRequirement already satisfied: numpy>=1.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-07ef3d41-7521-481a-8567-27fae047cd8b/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.4)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\nCollecting thinc<8.2.0,>=8.1.8\n  Downloading thinc-8.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (919 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 919.6/919.6 kB 27.8 MB/s eta 0:00:00\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.7)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (63.4.1)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-07ef3d41-7521-481a-8567-27fae047cd8b/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\nRequirement already satisfied: pathy>=0.10.0 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-07ef3d41-7521-481a-8567-27fae047cd8b/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.32.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-07ef3d41-7521-481a-8567-27fae047cd8b/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.67.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /databricks/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /databricks/python3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.9.14)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.11)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /databricks/python3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /databricks/python3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.1)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /databricks/python3/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\nBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py): started\n  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n  Created wheel for en-core-web-sm: filename=en_core_web_sm-3.5.0-py3-none-any.whl size=12803284 sha256=1b6668a965840c7018921b95adc67276a7ee7c416d5926723d470e961e287294\n  Stored in directory: /root/.cache/pip/wheels/88/70/a5/ea06bb10f3fe2c6bbfa0e36e3deca1adbf3648228490b2925c\nSuccessfully built en-core-web-sm\nInstalling collected packages: thinc, spacy, en-core-web-sm\n  Attempting uninstall: thinc\n    Found existing installation: thinc 8.1.7\n    Uninstalling thinc-8.1.7:\n      Successfully uninstalled thinc-8.1.7\n  Attempting uninstall: spacy\n    Found existing installation: spacy 3.4.4\n    Uninstalling spacy-3.4.4:\n      Successfully uninstalled spacy-3.4.4\nSuccessfully installed en-core-web-sm-3.5.0 spacy-3.5.4 thinc-8.1.12\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee8c6c9f-7b06-4f75-8dbc-254d2441a032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['parser'])\n",
    "# Excluding financially reavant stopwords\n",
    "nlp.Defaults.stop_words -= {\"bottom\", \"top\", \"Bottom\", \"Top\", \"call\", \"down\"}\n",
    "nlp.max_length = 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0058fae4-fa77-4c5a-9b1a-0fecd389340f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf96db65-93ea-46f2-9f64-6ab5733df782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:01:37\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mtext_preprocessing.py:55\u001B[0m | Loading SpaCy model: en_core_web_sm\n\u001B[32m2024-12-13 15:01:38\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:62\u001B[0m | Excluded stop words: ['bottom', 'top', 'Bottom', 'Top', 'call']\n\u001B[32m2024-12-13 15:01:38\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mtext_preprocessing.py:64\u001B[0m | SpaCy model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "nlp_ref = initialize_spacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c61d4fa-a303-48b8-a7c0-42a6ece1ea63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Topics dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "097f00b9-c94b-4951-b94a-b5eab3148cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Loading topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5864ffc7-3fa3-4d8f-a860-fe31e5760a7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "client ='MASS'\n",
    "version = 'v0'\n",
    "use_case = ''\n",
    "match_df_v0_old = pd.read_csv(f\"/dbfs/mnt/access_work/UC25/Topic Modeling/NLI Models/{client}_{use_case}_dictionaries_{version}.csv\")\n",
    "match_df_v0_ref = match_df_v0_old.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db4c7aa-44b0-4de8-a1d1-940a48c58ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Match keyword list explosion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbecf66a-203c-4cf5-8421-fa147652463e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5566f8d6-026d-486b-8877-d65866cdd5ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subtopic</th>\n",
       "      <th>Refined Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Creation</td>\n",
       "      <td>recruit_aggressively</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Creation</td>\n",
       "      <td>hire_salesperson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Creation</td>\n",
       "      <td>hire_rep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Creation</td>\n",
       "      <td>hire_engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Creation</td>\n",
       "      <td>hire_aggressively</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Subtopic      Refined Keywords\n",
       "0  Job Creation  recruit_aggressively\n",
       "0  Job Creation      hire_salesperson\n",
       "0  Job Creation              hire_rep\n",
       "0  Job Creation         hire_engineer\n",
       "0  Job Creation     hire_aggressively"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_df_v0_old['Refined Keywords'] = match_df_v0_old['Refined Keywords'].apply(ast.literal_eval)\n",
    "match_df_old = match_df_v0_old[['Subtopic','Refined Keywords']].explode(column='Refined Keywords')\n",
    "\n",
    "match_df_old.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac62d3f9-9992-40ff-8ef8-671b4701c006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918c1639-51fb-4bfc-99df-4f3d9cbcce8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:01:08\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'Refined Keywords' to create 'Refined Keywords'.\n\u001B[32m2024-12-13 15:01:08\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'Refined Keywords'.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subtopic</th>\n",
       "      <th>Refined Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Creation</td>\n",
       "      <td>recruit_aggressively</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Creation</td>\n",
       "      <td>hire_salesperson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Creation</td>\n",
       "      <td>hire_rep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Creation</td>\n",
       "      <td>hire_engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Creation</td>\n",
       "      <td>hire_aggressively</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Subtopic      Refined Keywords\n",
       "0  Job Creation  recruit_aggressively\n",
       "0  Job Creation      hire_salesperson\n",
       "0  Job Creation              hire_rep\n",
       "0  Job Creation         hire_engineer\n",
       "0  Job Creation     hire_aggressively"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:01:08\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n"
     ]
    }
   ],
   "source": [
    "match_df_v0_ref = df_apply_transformations(match_df_v0_ref, [('Refined Keywords', 'Refined Keywords', ast.literal_eval)])\n",
    "match_df_ref = match_df_v0_ref[['Subtopic','Refined Keywords']].explode(column='Refined Keywords')\n",
    "\n",
    "match_df_ref.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8fc3dfd-d99c-4851-b7d2-0ca0c5158683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert match_df_old.equals(match_df_ref), \"The DataFrames are not identical.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "342a423d-7fd6-4549-bb13-e32c878215ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Common code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c15b1f6-4797-4265-947a-3f7a7545f771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This process involves filtering and transforming data to handle negation logic within a dataset. It begins by isolating rows with non-null negation information, then parses and expands these entries to ensure each negation term is treated individually. A flag is added to indicate the presence of negation, and the data is reorganized to align with a standard format. Finally, the processed negation data is combined with the main dataset, integrating both negation and refined keyword information for comprehensive analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b024716-7421-4862-8fd1-efcec32fc0f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:01:13\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'Negation' to create 'Negation'.\n\u001B[32m2024-12-13 15:01:13\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'Negation'.\n\u001B[32m2024-12-13 15:01:13\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n"
     ]
    }
   ],
   "source": [
    "match_df_negate = match_df_v0_ref[~match_df_v0_ref['Negation'].isna()][['Subtopic', 'Negation']]#.apply(lambda x: ast.literal_eval(x['Negation']), axis=1)#.explode(column = 'Negation')\n",
    "match_df_negate = df_apply_transformations(match_df_negate, [('Negation', 'Negation', ast.literal_eval)])\n",
    "match_df_negate = match_df_negate.explode(column = 'Negation')\n",
    "match_df_negate['negate'] = True\n",
    "match_df_negate = match_df_negate.rename(columns = {'Subtopic': 'label', 'Negation': 'match'})\n",
    "match_df_ref['negate'] = False\n",
    "match_df_ref = match_df_ref.rename(columns={'Subtopic':'label', 'Refined Keywords':'match'})\n",
    "match_df_ref = pd.concat([match_df_ref, match_df_negate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c3a97b-b445-46e6-b832-a83c30e4c672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### create topic dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "863c8605-049d-4457-8c9a-0e58b50d5fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This process involves tokenizing and categorizing words from a dataset into different sets based on their linguistic properties and context. The matchTokenize function processes text to identify and lower-case proper nouns and significant words, excluding stop words, punctuation, and numbers. The get_match_set_old function then organizes these tokens into unigrams, bigrams, and phrases, creating distinct sets for each based on their structure and composition. Finally, the code constructs dictionaries to map topics to their corresponding word sets and negation terms, ensuring that each topic's keywords and negations are systematically categorized for further analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7b38c8-a031-4b86-b52d-ca0739d166bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8786c26-9294-4921-ae1a-fea06845ab68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "match_df_old = match_df_ref.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02989786-a0e3-4bdd-887a-12b9d0ad08ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def matchTokenize(doc):\n",
    "  ret = []\n",
    "  for ent in nlp(doc):\n",
    "    if ent.pos_ == 'PROPN' or ent.text[0].isupper():\n",
    "      ret.append(ent.text.lower())\n",
    "    #  print(ent.text.lower())\n",
    "    #  print(ent.text)\n",
    "      continue\n",
    "    if not ent.is_stop and not ent.is_punct and ent.pos_ != 'NUM':\n",
    "      ret.append(ent.lemma_.lower())\n",
    "  return ret\n",
    "\n",
    "def get_match_set_old(matches):\n",
    "  \n",
    "  bigrams = set([word.lower() for word in matches if len(word.split('_'))==2] + [word.lower().replace(\" \", '_') for word in matches if len(word.split(' '))==2] + ['_'.join(matchTokenize(word)) for word in matches if len(word.split(' '))==2])\n",
    " \n",
    "  unigrams = set([matchTokenize(match)[0] for match in matches if ('_' not in match) and (len(match.split(' '))==1)] + [match.lower() for match in matches if ('_' not in match) and (len(match.split(' '))==1)])\n",
    "\n",
    "  # phrases = set([phrase.lower() for phrase in matches if len(phrase.split(\" \"))>2] + [' '.join(matchTokenize(phrase)) for phrase in matches if len(phrase.split(\" \"))>2])\n",
    "\n",
    "  # Phrase matching correction\n",
    "  phrases = [phrase.lower() for phrase in matches if len(phrase.split(\" \"))>2]\n",
    "  \n",
    "  #print(matches)\n",
    "  #print(unigrams, bigrams, phrases)\n",
    "  return {'original': matches, 'unigrams': unigrams, 'bigrams': bigrams, 'phrases': phrases}\n",
    "  \n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "word_set_dict_old = {topic.replace(' ', '_').upper() : get_match_set_old(match_df_old[(match_df_old['label']==topic) & (match_df_old['negate']==False)]['match'].values) for topic in match_df_old['label'].unique()}\n",
    "\n",
    "negate_dict_old = {topic.replace(' ', '_').upper() : [word.lower() for word in match_df_old[(match_df_old['label']==topic) & (match_df_old['negate']==True)]['match'].values.tolist()] for topic in match_df_old['label'].unique()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09971c2-1617-45ed-b2a6-3c2dee97aef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "word_set_dict_old = {topic.replace(' ', '_').upper() : get_match_set_old(match_df_old[(match_df_old['label']==topic) & (match_df_old['negate']==False)]['match'].values) for topic in match_df_old['label'].unique()}\n",
    "\n",
    "negate_dict_old = {topic.replace(' ', '_').upper() : [word.lower() for word in match_df_old[(match_df_old['label']==topic) & (match_df_old['negate']==True)]['match'].values.tolist()] for topic in match_df_old['label'].unique()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b8beebf-7774-4288-ba68-88147db413c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee8b7fdb-5f65-427e-9110-585703fc61e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['recruit_aggressively', 'hire_salesperson', 'hire_rep', 'hire_engineer', 'hire_aggressively', 'hire_onboarde', 'recruiting_effort', 'add_people', 'expand_workforce', 'actively_recruit', 'hire_people', 'recruit_hire', 'recruit', 'aggressive_hiring', 'hire'], 'unigrams': {'recruit', 'hire'}, 'bigrams': {'recruit_aggressively', 'actively_recruit', 'aggressive_hiring', 'recruit_hire', 'add_people', 'hire_aggressively', 'hire_rep', 'hire_onboarde', 'recruiting_effort', 'expand_workforce', 'hire_salesperson', 'hire_people', 'hire_engineer'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'JOB_CREATION': 15 positive matches, 45 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 2 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 2 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 2 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['hiring_freeze', 'freeze_hire', 'recruitment_freeze', 'freeze_hiring', 'slow_hiring', 'pause_hiring', 'Hiring Freeze', 'flat head count', 'head count flat', 'hire freeze', 'furlough', 'workforce_reduction', 'layoff_furlough', 'furlough_layoff', 'temporary_layoff', 'furlough_temporary', 'permanent_layoff', 'tech_layoff', 'temporary_furlough', 'furlough_lay', 'employee_furlough', 'voluntary_departure', 'lay_furlough', 'branch_closure', 'furlough_employee', 'voluntary_retirement', 'temporarily_lay', 'furlough_staff', 'furlough_salary', 'Layoffs', 'reducing head count', 'reduce_workforce', 'reduction in force', 'reducing staff levels', 'reducing staffs', 'reduction in new hires', 'let people go', 'head count reduction', 'reduce_nonessential'], 'unigrams': {'furlough', 'layoffs'}, 'bigrams': {'reduce_nonessential', 'pause_hiring', 'lay_furlough', 'reduce_staff', 'furlough_employee', 'layoff_furlough', 'temporary_layoff', 'furlough_salary', 'voluntary_retirement', 'tech_layoff', 'hiring_freeze', 'employee_furlough', 'freeze_hire', 'furlough_lay', 'hire_freeze', 'furlough_layoff', 'slow_hiring', 'branch_closure', 'reducing_staffs', 'reduce_workforce', 'recruitment_freeze', 'temporary_furlough', 'permanent_layoff', 'furlough_staff', 'temporarily_lay', 'furlough_temporary', 'voluntary_departure', 'freeze_hiring', 'workforce_reduction'}, 'phrases': ['flat head count', 'head count flat', 'reducing head count', 'reduction in force', 'reducing staff levels', 'reduction in new hires', 'let people go', 'head count reduction']}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'JOB_REDUCTION': 39 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 2 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 2 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['labor_constraint', 'staffing_shortage', 'staff_shortage', 'labor_scarcity', 'tight_labor', 'labor_tightness', 'scarcity_labor', 'Labor Shortage', 'labor_strike', 'labor_availability', 'staff_turnover', 'Staffing Challenges'], 'unigrams': set(), 'bigrams': {'staffing_challenges', 'labor_availability', 'staffing_shortage', 'staff_shortage', 'tight_labor', 'staff_turnover', 'labor_shortage', 'labor_scarcity', 'scarcity_labor', 'labor_tightness', 'labor_strike', 'labor_constraint'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'LABOR_SHORTAGE': 12 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['automate', 'automation_robotic', 'automatization', 'intelligent_automation', 'automate_process', 'workflow_automation', 'robotic_automation', 'fully_automate', 'automization', 'automate_workflow', 'automated', 'automation_workflow', 'highly_automate', 'automation_digitalization', 'streamline_process', 'replace_manual', 'automation_digitization', 'digitization_automation', 'automating', 'hyper_automation', 'intelligence_automation', 'Automation'], 'unigrams': {'automated', 'automating', 'automate', 'automatization', 'automization', 'automation'}, 'bigrams': {'intelligence_automation', 'automation_digitization', 'automation_robotic', 'intelligent_automation', 'automation_digitalization', 'automation_workflow', 'automate_workflow', 'fully_automate', 'digitization_automation', 'workflow_automation', 'robotic_automation', 'automate_process', 'highly_automate', 'streamline_process', 'hyper_automation', 'replace_manual'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'AUTOMATION': 22 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['skill_training', 'training', 'training_program', 'recruiting_training', 'reskilling', 'educational_program', 'knowledge_sharing'], 'unigrams': {'reskilling', 'train', 'reskille', 'training'}, 'bigrams': {'educational_program', 'skill_training', 'training_program', 'recruiting_training', 'knowledge_sharing'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'EDUCATION_TRAINING_PROGRAM': 7 positive matches, 7 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 2 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 2 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 2 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['wage pressure', 'wage increases', 'wage negotiations', 'wage_inflation', 'labor cost inflation', 'labor cost increases'], 'unigrams': set(), 'bigrams': {'wage_inflation', 'wage_increases', 'wage_increase', 'wage_negotiation', 'wage_negotiations', 'wage_pressure'}, 'phrases': ['labor cost inflation', 'labor cost increases']}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'WAGE_INCREASES': 6 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['face_challenging', 'amid_challenging', 'challenging_uncertain', 'amid_challenge', 'volatile_macroeconomic', 'increasingly_challenging', 'midst_challenging', 'amidst_challenge', 'challenging', 'challenging_macroeconomic', 'amidst_challenging', 'volatile_uncertain', 'volatile_macro', 'uncertain_geopolitical', 'extremely_challenging', 'exceptionally_challenging', 'despite_turbulent', 'amid_uncertain', 'uncertain_volatile', 'amidst_uncertain', 'fluid_uncertain', 'uncertain_macroeconomic', 'challenging_macro', 'despite_challenging', 'navigate_volatile'], 'unigrams': {'challenge', 'challenging'}, 'bigrams': {'despite_turbulent', 'challenging_uncertain', 'amidst_challenging', 'amidst_challenge', 'amid_challenging', 'face_challenging', 'increasingly_challenging', 'despite_challenging', 'volatile_macroeconomic', 'amid_challenge', 'fluid_uncertain', 'amidst_uncertain', 'navigate_volatile', 'volatile_uncertain', 'extremely_challenging', 'volatile_macro', 'uncertain_geopolitical', 'uncertain_volatile', 'exceptionally_challenging', 'challenging_macroeconomic', 'uncertain_macroeconomic', 'challenging_macro', 'amid_uncertain', 'midst_challenging'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'CHALLEGING_UNCERTAIN_VOLATILE': 25 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['cost_saving', 'cost_cutting', 'cost_takeout', 'footprint_optimization', 'rightsizing', 'saving_initiative', 'footprint_rationalization', 'organizational_restructuring', 'restructuring', 'workforce_reduction', 'cost_containment', 'restructure_action', 'organizational_realignment', 'reorganization'], 'unigrams': {'reorganization', 'restructuring', 'restructure', 'rightsizing', 'rightsize'}, 'bigrams': {'cost_containment', 'footprint_rationalization', 'organizational_realignment', 'cost_cutting', 'footprint_optimization', 'cost_takeout', 'organizational_restructuring', 'restructure_action', 'cost_saving', 'workforce_reduction', 'saving_initiative'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'COSTCUTTING': 14 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['disruption', 'logistical_challenge', 'capacity_constraint', 'labor_constraint', 'supply_constraint', 'logistical_issue', 'labor_shortage', 'supply_shortage', 'disturbance', 'logistic_constraint', 'logistical_constraint', 'temporary_disruption', 'logistical_disruption', 'component_shortage', 'shipping_delay', 'logistic_bottleneck', 'port_congestion', 'container_shortage', 'semiconductor_shortage', 'logistical_problem', 'labor_availability', 'staffing_shortage', 'staff_shortage', 'congestion_port', 'labor_tightness', 'tightness_supply'], 'unigrams': {'disruption', 'disturbance'}, 'bigrams': {'labor_availability', 'supply_shortage', 'shipping_delay', 'port_congestion', 'container_shortage', 'capacity_constraint', 'temporary_disruption', 'component_shortage', 'staffing_shortage', 'logistical_challenge', 'staff_shortage', 'logistical_constraint', 'logistic_bottleneck', 'labor_shortage', 'semiconductor_shortage', 'logistical_issue', 'tightness_supply', 'supply_constraint', 'congestion_port', 'logistical_disruption', 'logistical_problem', 'logistic_constraint', 'labor_tightness', 'labor_constraint'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'DISRUPTION': 26 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['inflation', 'inflationary', 'commodity_inflation', 'rise_input', 'inflationary_environment', 'inflationary_pressure'], 'unigrams': {'inflationary', 'inflation'}, 'bigrams': {'commodity_inflation', 'inflationary_pressure', 'inflationary_environment', 'rise_input'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'INFLATION': 6 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['restock', 'inventory_stocking', 'replenish_stock', 'restocking', 'replenishment_order', 'replenish_inventory', 'rebuild_inventory', 'inventory_replenishment', 'inventory_restock'], 'unigrams': {'restock', 'restocking'}, 'bigrams': {'rebuild_inventory', 'inventory_replenishment', 'replenishment_order', 'inventory_stocking', 'replenish_inventory', 'inventory_restock', 'replenish_stock'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'RESTOCKING': 9 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['ample_liquidity', 'plenty_liquidity', 'sufficient_liquidity', 'adequate_liquidity', 'ample_capacity', 'dry_powder', 'financial_flexibility', 'available_liquidity', 'liquidity_headroom', 'liquidity_cushion'], 'unigrams': set(), 'bigrams': {'ample_liquidity', 'liquidity_headroom', 'liquidity_cushion', 'ample_capacity', 'dry_powder', 'financial_flexibility', 'sufficient_liquidity', 'plenty_liquidity', 'available_liquidity', 'adequate_liquidity'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'LIQUIDITY': 10 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['political_uncertainty', 'trade_war', 'geopolitical_tension', 'protectionism', 'trade_conflict', 'geopolitical_issue', 'geopolitic', 'trade_tension', 'geopolitical_uncertainty', 'geopolitical_risk', 'political_tension', 'political_turmoil', 'trade_dispute', 'geopolitical_instability', 'geopolitical_conflict', 'trade_friction', 'tariff_war', 'geopolitical_unrest', 'military_conflict', 'geopolitical_environment', 'u.s.-china_trade', 'geopolitical_turmoil', 'heighten_geopolitical'], 'unigrams': {'geopolitic', 'protectionism'}, 'bigrams': {'political_turmoil', 'geopolitical_unrest', 'geopolitical_issue', 'political_tension', 'geopolitical_conflict', 'heighten_geopolitical', 'geopolitical_environment', 'tariff_war', 'geopolitical_instability', 'trade_conflict', 'u.s.-china_trade', 'trade_friction', 'trade_dispute', 'trade_war', 'geopolitical_uncertainty', 'geopolitical_risk', 'military_conflict', 'geopolitical_turmoil', 'geopolitical_tension', 'trade_tension', 'political_uncertainty'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'GEOPOL_UNCERTAINTY': 23 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['problem', 'frustration', 'roadblock', 'inconvenience', 'obstacle', 'bottleneck', 'headache', 'overhang', 'difficulty', 'discomfort', 'hassle', 'distraction', 'hiccup', 'friction', 'complication', 'trouble', 'issue'], 'unigrams': {'hassle', 'complication', 'bottleneck', 'problem', 'headache', 'discomfort', 'frustration', 'roadblock', 'distraction', 'difficulty', 'hiccup', 'friction', 'overhang', 'inconvenience', 'trouble', 'obstacle', 'issue'}, 'bigrams': set(), 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'PROBLEM': 17 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['meltdown', 'crash', 'terrorist_attack', 'horrible', 'perfect_storm', 'black_swan', 'calamity', 'tsunami', 'horrific', 'collapse', 'panic', 'craziness', 'disastrous', 'carnage', 'credit_crunch', 'great_recession', 'crypto_winter', 'deep_recession'], 'unigrams': {'horrible', 'crash', 'collapse', 'disastrous', 'calamity', 'panic', 'meltdown', 'carnage', 'craziness', 'horrific', 'tsunami'}, 'bigrams': {'deep_recession', 'crypto_winter', 'terrorist_attack', 'black_swan', 'credit_crunch', 'great_recession', 'perfect_storm'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'RECESSION_MELTDOWN_BLACK_SWAN': 18 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['economic_downturn', 'downturn', 'economic_cycle', 'recession', 'severe_downturn', 'cyclical_downturn', 'recessionary_environment', 'economic_shock', 'prolonged_downturn', 'deep_recession', 'recessionary_period', 'recessionary'], 'unigrams': {'recessionary', 'recession', 'downturn'}, 'bigrams': {'recessionary_environment', 'severe_downturn', 'recessionary_period', 'deep_recession', 'cyclical_downturn', 'economic_shock', 'economic_cycle', 'prolonged_downturn', 'economic_downturn'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'ECONOMIC_DOWNTURN': 12 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['shortage', 'logistic_constraint', 'component_shortage', 'supply_shortage', 'supply_constraint', 'lack_availability', 'difficulty_procure', 'bottleneck', 'shipping_delay', 'delay_shipment', 'limited_availability', 'logjam'], 'unigrams': {'shortage', 'logjam', 'bottleneck'}, 'bigrams': {'difficulty_procure', 'component_shortage', 'lack_availability', 'supply_shortage', 'supply_constraint', 'delay_shipment', 'shipping_delay', 'logistic_constraint', 'limited_availability'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT': 12 positive matches, 13 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:225\u001B[0m | Tokenized matched words into 1 tokens.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mmatch_operations.py:78\u001B[0m | Generated match patterns: {'original': ['logistic_constraint', 'component_shortage', 'chip_shortage', 'supply_constraint', 'semiconductor_shortage', 'shortage_semiconductor', 'capacity_constraint', 'microchip_shortage', 'shortage_chip', 'supply_tightness', 'port_congestion', 'factory_shutdown', 'logistic_bottleneck', 'container_shortage', 'cpu_shortage', 'transportation_bottleneck', 'chipset_shortage', 'logistical_bottleneck', 'logistical_disruption', 'semi_shortage', 'shortage_container', 'logistical_issue', 'lack_availability', 'difficulty_procure', 'bottleneck', 'shipping_delay', 'delay_shipment', 'limited_availability', 'logjam'], 'unigrams': {'logjam', 'bottleneck'}, 'bigrams': {'shortage_container', 'difficulty_procure', 'transportation_bottleneck', 'shipping_delay', 'port_congestion', 'container_shortage', 'capacity_constraint', 'factory_shutdown', 'component_shortage', 'delay_shipment', 'limited_availability', 'logistic_bottleneck', 'chip_shortage', 'shortage_chip', 'semiconductor_shortage', 'logistical_issue', 'lack_availability', 'supply_constraint', 'logistical_bottleneck', 'microchip_shortage', 'supply_tightness', 'semi_shortage', 'logistical_disruption', 'shortage_semiconductor', 'cpu_shortage', 'logistic_constraint', 'chipset_shortage'}, 'phrases': []}\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:82\u001B[0m | Processed topic 'SUPPLY_CHAIN_SHORTAGE': 29 positive matches, 0 negative matches.\n\u001B[32m2024-12-13 15:01:49\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:88\u001B[0m | Created topic dictionaries successfully.\n"
     ]
    }
   ],
   "source": [
    "word_set_dict_ref, negate_dict_ref = create_topic_dict(match_df_ref, nlp_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a8bda4-e0cf-440b-bd42-7f62f83c85ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert all(\n",
    "    key in word_set_dict_ref and\n",
    "    all(np.array_equal(word_set_dict_old[key][sub_key], word_set_dict_ref[key][sub_key])\n",
    "        for sub_key in word_set_dict_old[key])\n",
    "    for key in word_set_dict_old\n",
    "), \"The dictionaries are not equal.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac753fb-073b-4e7e-bdbd-4a71dd2dbadb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replace separator in dicts with underscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ca4002-dd11-41b5-ab92-9b63f0501767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This process refines a dictionary of negation terms by converting bigrams (words with underscores) into space-separated phrases, ensuring consistent formatting across all entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b3b4d07-dca2-430a-b74e-90fdc9f566b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### old "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022b294e-f21d-4222-bfd2-f6e56f6b0c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "negate_dict1_old = {k: [] for k in negate_dict_old.keys()}\n",
    "for k, v in negate_dict_old.items():\n",
    "  for word in v:\n",
    "    if len(word.split('_'))==2:\n",
    "      new_word = ' '.join(word.split('_'))\n",
    "      negate_dict1_old[k].append(new_word)\n",
    "    else:\n",
    "      negate_dict1_old[k].append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad54b1ed-530b-4c9c-9efc-92bcd0c3e969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322fbdf1-d9a7-40f9-8daa-11499e555e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'not hiring' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'not hire' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'reduce additional hiring' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'hiring_freeze' to 'hiring freeze' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'freeze_hire' to 'freeze hire' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'recruitment_freeze' to 'recruitment freeze' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'freeze_hiring' to 'freeze hiring' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'slow_hiring' to 'slow hiring' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'pause_hiring' to 'pause hiring' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'hiring freeze' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'stabilize_workforce' to 'stabilize workforce' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'flat head count' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'head count flat' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'hire freeze' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'furlough' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'workforce_reduction' to 'workforce reduction' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'layoff_furlough' to 'layoff furlough' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'furlough_layoff' to 'furlough layoff' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'temporary_layoff' to 'temporary layoff' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'furlough_temporary' to 'furlough temporary' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'permanent_layoff' to 'permanent layoff' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'downsizing' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'tech_layoff' to 'tech layoff' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'temporary_furlough' to 'temporary furlough' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'furlough_lay' to 'furlough lay' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'employee_furlough' to 'employee furlough' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'downsize' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'voluntary_departure' to 'voluntary departure' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'lay_furlough' to 'lay furlough' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'branch_closure' to 'branch closure' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'furlough_employee' to 'furlough employee' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'voluntary_retirement' to 'voluntary retirement' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'temporarily_lay' to 'temporarily lay' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'furlough_staff' to 'furlough staff' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'furlough_salary' to 'furlough salary' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'layoffs' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'reducing head count' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:141\u001B[0m | Transformed 'reduce_workforce' to 'reduce workforce' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'reduction in force' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'reducing staff levels' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'reducing staffs' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'reduction in new hires' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'let people go' in category 'JOB_CREATION'.\n\u001B[32m2024-12-13 15:01:58\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mreport_generation.py:144\u001B[0m | No transformation for 'head count reduction' in category 'JOB_CREATION'.\n"
     ]
    }
   ],
   "source": [
    "negate_dict1_ref = replace_separator_in_dict_words(negate_dict_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e564e6b-e246-4e02-848d-03098d1974ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert all(np.array_equal(negate_dict1_old[key], negate_dict1_ref[key]) for key in negate_dict1_ref), \"The dictionaries are not equal.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f3fcaa8-0cb2-4ff4-876d-9d3bc6cce5d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Genearte Topic report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8738edaa-9182-4660-b6e0-10dbe4bdd960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code processes text data to identify and count occurrences of specific words and phrases, using tokenization and n-gram analysis. It begins by tokenizing text into unigrams and bigrams, then matches these against predefined sets of keywords and phrases, while considering negation terms to suppress certain matches. The results are stored in a DataFrame, where each topic's total and statistical match counts are calculated. Additional statistics, such as relevance and sentiment scores, are derived from these counts to provide insights into the text's content and sentiment. The process leverages Dask for parallel computation to efficiently handle large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fad4bbf2-d961-4863-bfbe-24fa89f967c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ef093e-88bf-491d-a441-1b5b95931be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def wordTokenize(doc):\n",
    "  \n",
    "  return [ent.lemma_.lower() for ent in nlp(doc) if not ent.is_stop and not ent.is_punct and ent.pos_ != 'NUM']\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "  return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "\n",
    "\n",
    "def match_count_lowStat(texts, match_sets, phrases = True, suppress = None):\n",
    "\n",
    "  count_dict = {label : {matchw: 0 for matchw in match_set['unigrams'].union(match_set['bigrams']) } for label, match_set in match_sets.items()}\n",
    "  total_counts = {label: [] for label in match_sets.keys()}\n",
    "\n",
    "  for text in texts:\n",
    "    \n",
    "    counted = {label: 0 for label in match_sets.keys()}\n",
    "    unigrams = wordTokenize(text)\n",
    "    bigrams = ['_'.join(g) for g in find_ngrams(unigrams, 2)]\n",
    "    \n",
    "    text = text.lower()\n",
    "    for label, match_set in match_sets.items(): \n",
    "      \n",
    "      if any(item in text for item in suppress[label]):\n",
    "        counted[label] += 0\n",
    "        continue\n",
    "        \n",
    "      for word in unigrams:\n",
    "        if word in match_set['unigrams']:\n",
    "          count_dict[label][word]+=1\n",
    "          counted[label] += 1\n",
    "\n",
    "      for word in bigrams:\n",
    "        if word in match_set['bigrams']:\n",
    "          count_dict[label][word]+=1\n",
    "          counted[label] += 1\n",
    "      \n",
    "      if phrases:\n",
    "        if any(phrase in text for phrase in match_set['phrases']):\n",
    "          counted[label] += 1\n",
    "          continue\n",
    "\n",
    "    for label in match_sets.keys():\n",
    "      \n",
    "      total_counts[label].append(counted[label])\n",
    "\n",
    "    \n",
    "  return {label : {'total': total_counts[label], 'stats' : count_dict[label]} for label in match_sets.keys()}\n",
    "\n",
    "def sentscore(a, b, weight = True):\n",
    "  \n",
    "  # number of relevant sentences\n",
    "      \n",
    "  length = len(a)\n",
    "  if length==0:\n",
    "      return None\n",
    "  if length!=len(b):\n",
    "      return None\n",
    "  num = len([x for x in a if x>0])\n",
    "  \n",
    "  if num==0:\n",
    "      return None\n",
    "   \n",
    "  if weight==True:\n",
    "    return np.dot(a,b)/num\n",
    "  else:\n",
    "    return np.dot([1 if x>0 else 0 for x in a], b)/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50e81f5-9297-4f8f-ae8a-07d933e599b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r[                                        ] | 0% Completed | 245.39 us\r[                                        ] | 0% Completed | 101.53 ms\r[                                        ] | 0% Completed | 206.24 ms\r[                                        ] | 0% Completed | 310.24 ms\r[                                        ] | 0% Completed | 411.70 ms\r[                                        ] | 0% Completed | 515.56 ms\r[                                        ] | 0% Completed | 618.28 ms\r[                                        ] | 0% Completed | 722.38 ms\r[                                        ] | 0% Completed | 825.95 ms\r[                                        ] | 0% Completed | 930.11 ms\r[                                        ] | 0% Completed | 1.04 s\r[                                        ] | 0% Completed | 1.14 s\r[                                        ] | 0% Completed | 1.24 s\r[                                        ] | 0% Completed | 1.35 s\r[                                        ] | 0% Completed | 1.46 s\r[                                        ] | 0% Completed | 1.56 s\r[                                        ] | 0% Completed | 1.66 s\r[                                        ] | 0% Completed | 1.76 s\r[                                        ] | 0% Completed | 1.87 s\r[                                        ] | 0% Completed | 1.97 s\r[                                        ] | 0% Completed | 2.08 s\r[                                        ] | 0% Completed | 2.18 s\r[                                        ] | 0% Completed | 2.28 s\r[                                        ] | 0% Completed | 2.39 s\r[                                        ] | 0% Completed | 2.49 s\r[                                        ] | 0% Completed | 2.59 s\r[                                        ] | 0% Completed | 2.70 s\r[                                        ] | 0% Completed | 2.81 s\r[                                        ] | 0% Completed | 2.91 s\r[                                        ] | 0% Completed | 3.01 s\r[                                        ] | 0% Completed | 3.11 s\r[                                        ] | 0% Completed | 3.22 s\r[                                        ] | 0% Completed | 3.32 s\r[                                        ] | 0% Completed | 3.42 s\r[                                        ] | 0% Completed | 3.54 s\r[                                        ] | 0% Completed | 3.64 s\r[                                        ] | 0% Completed | 3.75 s\r[                                        ] | 0% Completed | 3.85 s\r[                                        ] | 0% Completed | 3.95 s\r[                                        ] | 0% Completed | 4.05 s\r[                                        ] | 0% Completed | 4.15 s\r[                                        ] | 0% Completed | 4.26 s\r[                                        ] | 0% Completed | 4.36 s\r[                                        ] | 0% Completed | 4.47 s\r[                                        ] | 0% Completed | 4.57 s\r[                                        ] | 0% Completed | 4.67 s\r[                                        ] | 0% Completed | 4.77 s\r[                                        ] | 0% Completed | 4.88 s\r[                                        ] | 0% Completed | 4.98 s\r[                                        ] | 0% Completed | 5.08 s\r[                                        ] | 0% Completed | 5.19 s\r[                                        ] | 0% Completed | 5.29 s\r[                                        ] | 0% Completed | 5.39 s\r[                                        ] | 0% Completed | 5.50 s\r[                                        ] | 0% Completed | 5.60 s\r[                                        ] | 0% Completed | 5.71 s\r[                                        ] | 0% Completed | 5.81 s\r[                                        ] | 0% Completed | 5.91 s\r[                                        ] | 0% Completed | 6.01 s\r[                                        ] | 0% Completed | 6.12 s\r[                                        ] | 0% Completed | 6.22 s\r[                                        ] | 0% Completed | 6.32 s\r[                                        ] | 0% Completed | 6.43 s\r[                                        ] | 0% Completed | 6.53 s\r[                                        ] | 0% Completed | 6.63 s\r[                                        ] | 0% Completed | 6.73 s\r[                                        ] | 0% Completed | 6.84 s\r[                                        ] | 0% Completed | 6.94 s\r[                                        ] | 0% Completed | 7.05 s\r[                                        ] | 0% Completed | 7.15 s\r[                                        ] | 0% Completed | 7.26 s\r[                                        ] | 0% Completed | 7.36 s\r[                                        ] | 0% Completed | 7.46 s\r[                                        ] | 0% Completed | 7.57 s\r[                                        ] | 0% Completed | 7.67 s\r[                                        ] | 0% Completed | 7.77 s\r[                                        ] | 0% Completed | 7.87 s\r[                                        ] | 0% Completed | 7.98 s\r[                                        ] | 0% Completed | 8.09 s\r[                                        ] | 0% Completed | 8.19 s\r[                                        ] | 0% Completed | 8.29 s\r[                                        ] | 0% Completed | 8.39 s\r[                                        ] | 0% Completed | 8.50 s\r[                                        ] | 0% Completed | 8.60 s\r[                                        ] | 0% Completed | 8.70 s\r[                                        ] | 0% Completed | 8.81 s\r[                                        ] | 0% Completed | 8.91 s\r[                                        ] | 0% Completed | 9.02 s\r[                                        ] | 0% Completed | 9.12 s\r[                                        ] | 0% Completed | 9.22 s\r[                                        ] | 0% Completed | 9.33 s\r[                                        ] | 0% Completed | 9.43 s\r[                                        ] | 0% Completed | 9.53 s\r[                                        ] | 0% Completed | 9.64 s\r[                                        ] | 0% Completed | 9.74 s\r[                                        ] | 0% Completed | 9.85 s\r[                                        ] | 0% Completed | 9.95 s\r[                                        ] | 0% Completed | 10.05 s\r[                                        ] | 0% Completed | 10.16 s\r[                                        ] | 0% Completed | 10.26 s\r[                                        ] | 0% Completed | 10.37 s\r[                                        ] | 0% Completed | 10.47 s\r[                                        ] | 0% Completed | 10.57 s\r[                                        ] | 0% Completed | 10.68 s\r[                                        ] | 0% Completed | 10.78 s\r[                                        ] | 0% Completed | 10.89 s\r[                                        ] | 0% Completed | 10.99 s\r[                                        ] | 0% Completed | 11.10 s\r[                                        ] | 0% Completed | 11.20 s\r[                                        ] | 0% Completed | 11.31 s\r[                                        ] | 0% Completed | 11.41 s\r[                                        ] | 0% Completed | 11.51 s\r[                                        ] | 0% Completed | 11.62 s\r[                                        ] | 0% Completed | 11.72 s\r[                                        ] | 0% Completed | 11.82 s\r[                                        ] | 0% Completed | 11.93 s\r[                                        ] | 0% Completed | 12.03 s\r[                                        ] | 0% Completed | 12.13 s\r[                                        ] | 0% Completed | 12.24 s\r[                                        ] | 0% Completed | 12.34 s\r[                                        ] | 0% Completed | 12.44 s\r[                                        ] | 0% Completed | 12.54 s\r[                                        ] | 0% Completed | 12.65 s\r[                                        ] | 0% Completed | 12.75 s\r[                                        ] | 0% Completed | 12.86 s\r[                                        ] | 0% Completed | 12.96 s\r[                                        ] | 0% Completed | 13.06 s\r[                                        ] | 0% Completed | 13.17 s\r[                                        ] | 0% Completed | 13.27 s\r[                                        ] | 0% Completed | 13.38 s\r[                                        ] | 0% Completed | 13.48 s\r[                                        ] | 0% Completed | 13.58 s\r[                                        ] | 0% Completed | 13.69 s\r[                                        ] | 0% Completed | 13.79 s\r[                                        ] | 0% Completed | 13.89 s\r[                                        ] | 0% Completed | 14.00 s\r[                                        ] | 0% Completed | 14.10 s\r[                                        ] | 0% Completed | 14.20 s\r[                                        ] | 0% Completed | 14.31 s\r[                                        ] | 0% Completed | 14.41 s\r[                                        ] | 0% Completed | 14.51 s\r[                                        ] | 0% Completed | 14.62 s\r[                                        ] | 0% Completed | 14.72 s\r[                                        ] | 0% Completed | 14.82 s\r[                                        ] | 0% Completed | 14.92 s\r[                                        ] | 0% Completed | 15.03 s\r[                                        ] | 0% Completed | 15.13 s\r[                                        ] | 0% Completed | 15.24 s\r[                                        ] | 0% Completed | 15.35 s\r[                                        ] | 0% Completed | 15.45 s\r[                                        ] | 0% Completed | 15.55 s\r[                                        ] | 0% Completed | 15.65 s\r[                                        ] | 0% Completed | 15.76 s\r[                                        ] | 0% Completed | 15.86 s\r[                                        ] | 0% Completed | 15.97 s\r[                                        ] | 0% Completed | 16.07 s\r[                                        ] | 0% Completed | 16.17 s\r[                                        ] | 0% Completed | 16.28 s\r[                                        ] | 0% Completed | 16.38 s\r[                                        ] | 0% Completed | 16.49 s\r[                                        ] | 0% Completed | 16.59 s\r[                                        ] | 0% Completed | 16.69 s\r[                                        ] | 0% Completed | 16.80 s\r[                                        ] | 0% Completed | 16.91 s\r[                                        ] | 0% Completed | 17.01 s\r[                                        ] | 0% Completed | 17.11 s\r[                                        ] | 0% Completed | 17.22 s\r[                                        ] | 0% Completed | 17.32 s\r[                                        ] | 0% Completed | 17.42 s\r[                                        ] | 0% Completed | 17.53 s\r[                                        ] | 0% Completed | 17.63 s\r[                                        ] | 0% Completed | 17.74 s\r[                                        ] | 0% Completed | 17.84 s\r[                                        ] | 0% Completed | 17.95 s\r[                                        ] | 0% Completed | 18.05 s\r[                                        ] | 0% Completed | 18.15 s\r[                                        ] | 0% Completed | 18.26 s\r[                                        ] | 0% Completed | 18.36 s\r[                                        ] | 0% Completed | 18.46 s\r[                                        ] | 0% Completed | 18.57 s\r[                                        ] | 0% Completed | 18.67 s\r[                                        ] | 0% Completed | 18.78 s\r[                                        ] | 0% Completed | 18.89 s\r[                                        ] | 0% Completed | 18.99 s\r[                                        ] | 0% Completed | 19.09 s\r[                                        ] | 0% Completed | 19.20 s\r[                                        ] | 0% Completed | 19.30 s\r[                                        ] | 0% Completed | 19.40 s\r[                                        ] | 0% Completed | 19.51 s\r[                                        ] | 0% Completed | 19.61 s\r[                                        ] | 0% Completed | 19.72 s\r[                                        ] | 0% Completed | 19.82 s\r[                                        ] | 0% Completed | 19.92 s\r[                                        ] | 0% Completed | 20.03 s\r[                                        ] | 0% Completed | 20.13 s\r[                                        ] | 0% Completed | 20.23 s\r[                                        ] | 0% Completed | 20.34 s\r[                                        ] | 0% Completed | 20.44 s\r[                                        ] | 0% Completed | 20.54 s\r[                                        ] | 0% Completed | 20.65 s\r[                                        ] | 0% Completed | 20.75 s\r[                                        ] | 0% Completed | 20.86 s\r[                                        ] | 0% Completed | 20.96 s\r[                                        ] | 0% Completed | 21.06 s\r[                                        ] | 0% Completed | 21.17 s\r[                                        ] | 0% Completed | 21.27 s\r[                                        ] | 0% Completed | 21.37 s\r[                                        ] | 0% Completed | 21.48 s\r[                                        ] | 0% Completed | 21.58 s\r[                                        ] | 0% Completed | 21.69 s\r[                                        ] | 0% Completed | 21.79 s\r[                                        ] | 0% Completed | 21.89 s\r[                                        ] | 0% Completed | 22.00 s\r[                                        ] | 0% Completed | 22.10 s\r[                                        ] | 0% Completed | 22.21 s\r[                                        ] | 0% Completed | 22.31 s\r[                                        ] | 0% Completed | 22.42 s\r[                                        ] | 0% Completed | 22.52 s\r[                                        ] | 0% Completed | 22.62 s\r[                                        ] | 0% Completed | 22.73 s\r[                                        ] | 0% Completed | 22.83 s\r[                                        ] | 0% Completed | 22.94 s\r[                                        ] | 0% Completed | 23.04 s\r[                                        ] | 0% Completed | 23.15 s\r[                                        ] | 0% Completed | 23.25 s\r[                                        ] | 0% Completed | 23.35 s\r[                                        ] | 0% Completed | 23.46 s\r[                                        ] | 0% Completed | 23.56 s\r[                                        ] | 0% Completed | 23.66 s\r[                                        ] | 0% Completed | 23.77 s\r[                                        ] | 0% Completed | 23.87 s\r[                                        ] | 0% Completed | 23.97 s\r[                                        ] | 0% Completed | 24.08 s\r[                                        ] | 0% Completed | 24.18 s\r[                                        ] | 0% Completed | 24.29 s\r[                                        ] | 0% Completed | 24.39 s\r[                                        ] | 0% Completed | 24.49 s\r[                                        ] | 0% Completed | 24.59 s\r[                                        ] | 0% Completed | 24.70 s\r[                                        ] | 0% Completed | 24.81 s\r[                                        ] | 0% Completed | 24.91 s\r[                                        ] | 0% Completed | 25.02 s\r[                                        ] | 0% Completed | 25.12 s\r[                                        ] | 0% Completed | 25.22 s\r[                                        ] | 0% Completed | 25.33 s\r[                                        ] | 0% Completed | 25.43 s\r[                                        ] | 0% Completed | 25.54 s\r[                                        ] | 0% Completed | 25.64 s\r[                                        ] | 0% Completed | 25.74 s\r[                                        ] | 0% Completed | 25.85 s\r[                                        ] | 0% Completed | 25.95 s\r[                                        ] | 0% Completed | 26.06 s\r[                                        ] | 0% Completed | 26.16 s\r[                                        ] | 0% Completed | 26.26 s\r[                                        ] | 0% Completed | 26.36 s\r[                                        ] | 0% Completed | 26.47 s\r[                                        ] | 0% Completed | 26.57 s\r[                                        ] | 0% Completed | 26.68 s\r[                                        ] | 0% Completed | 26.82 s\r[                                        ] | 0% Completed | 26.94 s\r[                                        ] | 0% Completed | 27.04 s\r[                                        ] | 0% Completed | 27.15 s\r[                                        ] | 0% Completed | 27.25 s\r[                                        ] | 0% Completed | 27.35 s\r[                                        ] | 0% Completed | 27.46 s\r[                                        ] | 0% Completed | 27.56 s\r[                                        ] | 0% Completed | 27.66 s\r[                                        ] | 0% Completed | 27.77 s\r[                                        ] | 0% Completed | 27.87 s\r[                                        ] | 0% Completed | 27.97 s\r[                                        ] | 0% Completed | 28.08 s\r[                                        ] | 0% Completed | 28.18 s\r[                                        ] | 0% Completed | 28.29 s\r[                                        ] | 0% Completed | 28.39 s\r[                                        ] | 0% Completed | 28.49 s\r[                                        ] | 0% Completed | 28.59 s\r[                                        ] | 0% Completed | 28.70 s\r[                                        ] | 0% Completed | 28.80 s\r[                                        ] | 0% Completed | 28.90 s\r[                                        ] | 0% Completed | 29.01 s\r[                                        ] | 0% Completed | 29.11 s\r[                                        ] | 0% Completed | 29.21 s\r[                                        ] | 0% Completed | 29.32 s\r[                                        ] | 0% Completed | 29.42 s\r[                                        ] | 0% Completed | 29.52 s\r[                                        ] | 0% Completed | 29.63 s\r[                                        ] | 0% Completed | 29.73 s\r[                                        ] | 0% Completed | 29.83 s\r[                                        ] | 0% Completed | 29.94 s\r[                                        ] | 0% Completed | 30.04 s\r[                                        ] | 0% Completed | 30.14 s\r[                                        ] | 0% Completed | 30.24 s\r[                                        ] | 0% Completed | 30.35 s\r[                                        ] | 0% Completed | 30.45 s\r[                                        ] | 0% Completed | 30.56 s\r[                                        ] | 0% Completed | 30.67 s\r[                                        ] | 0% Completed | 30.77 s\r[                                        ] | 0% Completed | 30.87 s\r[                                        ] | 0% Completed | 30.97 s\r[                                        ] | 0% Completed | 31.08 s\r[                                        ] | 0% Completed | 31.18 s\r[                                        ] | 0% Completed | 31.28 s\r[                                        ] | 0% Completed | 31.39 s\r[                                        ] | 0% Completed | 31.49 s\r[                                        ] | 0% Completed | 31.59 s\r[                                        ] | 0% Completed | 31.69 s\r[                                        ] | 0% Completed | 31.80 s\r[                                        ] | 0% Completed | 31.90 s\r[                                        ] | 0% Completed | 32.00 s\r[                                        ] | 0% Completed | 32.11 s\r[                                        ] | 0% Completed | 32.21 s\r[                                        ] | 0% Completed | 32.32 s\r[                                        ] | 0% Completed | 32.42 s\r[                                        ] | 0% Completed | 32.53 s\r[                                        ] | 0% Completed | 32.63 s\r[                                        ] | 0% Completed | 32.74 s\r[                                        ] | 0% Completed | 32.84 s\r[                                        ] | 0% Completed | 32.94 s\r[                                        ] | 0% Completed | 33.05 s\r[                                        ] | 0% Completed | 33.15 s\r[                                        ] | 0% Completed | 33.25 s\r[                                        ] | 0% Completed | 33.35 s\r[                                        ] | 0% Completed | 33.46 s\r[                                        ] | 0% Completed | 33.56 s\r[                                        ] | 0% Completed | 33.66 s\r[                                        ] | 0% Completed | 33.77 s\r[                                        ] | 0% Completed | 33.88 s\r[                                        ] | 0% Completed | 33.98 s\r[                                        ] | 0% Completed | 34.08 s\r[                                        ] | 0% Completed | 34.18 s\r[                                        ] | 0% Completed | 34.29 s\r[                                        ] | 0% Completed | 34.39 s\r[                                        ] | 0% Completed | 34.49 s\r[                                        ] | 0% Completed | 34.60 s\r[                                        ] | 0% Completed | 34.70 s\r[                                        ] | 0% Completed | 34.81 s\r[                                        ] | 0% Completed | 34.91 s\r[                                        ] | 0% Completed | 35.01 s\r[                                        ] | 0% Completed | 35.11 s\r[                                        ] | 0% Completed | 35.22 s\r[                                        ] | 0% Completed | 35.32 s\r[                                        ] | 0% Completed | 35.42 s\r[                                        ] | 0% Completed | 35.53 s\r[                                        ] | 0% Completed | 35.63 s\r[                                        ] | 0% Completed | 35.73 s\r[                                        ] | 0% Completed | 35.84 s\r[                                        ] | 0% Completed | 35.94 s\r[                                        ] | 0% Completed | 36.04 s\r[                                        ] | 0% Completed | 36.15 s\r[                                        ] | 0% Completed | 36.25 s\r[                                        ] | 0% Completed | 36.36 s\r[                                        ] | 0% Completed | 36.46 s\r[                                        ] | 0% Completed | 36.57 s\r[                                        ] | 0% Completed | 36.67 s\r[                                        ] | 0% Completed | 36.78 s\r[                                        ] | 0% Completed | 36.88 s\r[                                        ] | 0% Completed | 36.99 s\r[                                        ] | 0% Completed | 37.09 s\r[                                        ] | 0% Completed | 37.19 s\r[                                        ] | 0% Completed | 37.30 s\r[                                        ] | 0% Completed | 37.40 s\r[                                        ] | 0% Completed | 37.50 s\r[                                        ] | 0% Completed | 37.61 s\r[                                        ] | 0% Completed | 37.71 s\r[                                        ] | 0% Completed | 37.82 s\r[                                        ] | 0% Completed | 37.92 s\r[                                        ] | 0% Completed | 38.03 s\r[                                        ] | 0% Completed | 38.13 s\r[                                        ]\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n 124.35 s\r[                                        ] | 0% Completed | 124.46 s\r[                                        ] | 0% Completed | 124.56 s\r[                                        ] | 0% Completed | 124.66 s\r[                                        ] | 0% Completed | 124.77 s\r[                                        ] | 0% Completed | 124.87 s\r[                                        ] | 0% Completed | 124.98 s\r[                                        ] | 0% Completed | 125.08 s\r[                                        ] | 0% Completed | 125.18 s\r[                                        ] | 0% Completed | 125.29 s\r[                                        ] | 0% Completed | 125.39 s\r[                                        ] | 0% Completed | 125.49 s\r[                                        ] | 0% Completed | 125.59 s\r[                                        ] | 0% Completed | 125.70 s\r[                                        ] | 0% Completed | 125.80 s\r[                                        ] | 0% Completed | 125.90 s\r[                                        ] | 0% Completed | 126.01 s\r[                                        ] | 0% Completed | 126.11 s\r[                                        ] | 0% Completed | 126.21 s\r[                                        ] | 0% Completed | 126.32 s\r[                                        ] | 0% Completed | 126.42 s\r[                                        ] | 0% Completed | 126.52 s\r[                                        ] | 0% Completed | 126.63 s\r[                                        ] | 0% Completed | 126.73 s\r[                                        ] | 0% Completed | 126.83 s\r[                                        ] | 0% Completed | 126.94 s\r[                                        ] | 0% Completed | 127.04 s\r[                                        ] | 0% Completed | 127.14 s\r[                                        ] | 0% Completed | 127.24 s\r[                                        ] | 0% Completed | 127.35 s\r[                                        ] | 0% Completed | 127.45 s\r[                                        ] | 0% Completed | 127.55 s\r[                                        ] | 0% Completed | 127.66 s\r[                                        ] | 0% Completed | 127.76 s\r[                                        ] | 0% Completed | 127.87 s\r[                                        ] | 0% Completed | 127.98 s\r[                                        ] | 0% Completed | 128.08 s\r[                                        ] | 0% Completed | 128.18 s\r[                                        ] | 0% Completed | 128.29 s\r[                                        ] | 0% Completed | 128.39 s\r[                                        ] | 0% Completed | 128.49 s\r[                                        ] | 0% Completed | 128.60 s\r[                                        ] | 0% Completed | 128.70 s\r[                                        ] | 0% Completed | 128.81 s\r[                                        ] | 0% Completed | 128.91 s\r[                                        ] | 0% Completed | 129.01 s\r[                                        ] | 0% Completed | 129.12 s\r[                                        ] | 0% Completed | 129.22 s\r[                                        ] | 0% Completed | 129.32 s\r[                                        ] | 0% Completed | 129.43 s\r[                                        ] | 0% Completed | 129.53 s\r[                                        ] | 0% Completed | 129.63 s\r[                                        ] | 0% Completed | 129.73 s\r[                                        ] | 0% Completed | 129.83 s\r[                                        ] | 0% Completed | 129.94 s\r[                                        ] | 0% Completed | 130.04 s\r[                                        ] | 0% Completed | 130.14 s\r[                                        ] | 0% Completed | 130.24 s\r[                                        ] | 0% Completed | 130.35 s\r[                                        ] | 0% Completed | 130.45 s\r[                                        ] | 0% Completed | 130.55 s\r[                                        ] | 0% Completed | 130.66 s\r[                                        ] | 0% Completed | 130.76 s\r[                                        ] | 0% Completed | 130.87 s\r[                                        ] | 0% Completed | 130.97 s\r[                                        ] | 0% Completed | 131.07 s\r[                                        ] | 0% Completed | 131.18 s\r[                                        ] | 0% Completed | 131.28 s\r[                                        ] | 0% Completed | 131.38 s\r[                                        ] | 0% Completed | 131.48 s\r[                                        ] | 0% Completed | 131.59 s\r[                                        ] | 0% Completed | 131.69 s\r[                                        ] | 0% Completed | 131.80 s\r[                                        ] | 0% Completed | 131.90 s\r[                                        ] | 0% Completed | 132.01 s\r[                                        ] | 0% Completed | 132.11 s\r[                                        ] | 0% Completed | 132.21 s\r[                                        ] | 0% Completed | 132.31 s\r[                                        ] | 0% Completed | 132.42 s\r[                                        ] | 0% Completed | 132.52 s\r[                                        ] | 0% Completed | 132.62 s\r[                                        ] | 0% Completed | 132.72 s\r[                                        ] | 0% Completed | 132.83 s\r[                                        ] | 0% Completed | 132.93 s\r[                                        ] | 0% Completed | 133.03 s\r[                                        ] | 0% Completed | 133.14 s\r[                                        ] | 0% Completed | 133.24 s\r[                                        ] | 0% Completed | 133.35 s\r[                                        ] | 0% Completed | 133.45 s\r[                                        ] | 0% Completed | 133.56 s\r[                                        ] | 0% Completed | 133.66 s\r[                                        ] | 0% Completed | 133.76 s\r[                                        ] | 0% Completed | 133.87 s\r[                                        ] | 0% Completed | 133.97 s\r[                                        ] | 0% Completed | 134.08 s\r[                                        ] | 0% Completed | 134.18 s\r[                                        ] | 0% Completed | 134.28 s\r[                                        ] | 0% Completed | 134.38 s\r[                                        ] | 0% Completed | 134.49 s\r[                                        ] | 0% Completed | 134.59 s\r[                                        ] | 0% Completed | 134.70 s\r[                                        ] | 0% Completed | 134.80 s\r[                                        ] | 0% Completed | 134.90 s\r[                                        ] | 0% Completed | 135.01 s\r[                                        ] | 0% Completed | 135.11 s\r[                                        ] | 0% Completed | 135.21 s\r[                                        ] | 0% Completed | 135.32 s\r[                                        ] | 0% Completed | 135.42 s\r[                                        ] | 0% Completed | 135.52 s\r[                                        ] | 0% Completed | 135.63 s\r[                                        ] | 0% Completed | 135.73 s\r[                                        ] | 0% Completed | 135.83 s\r[                                        ] | 0% Completed | 135.93 s\r[                                        ] | 0% Completed | 136.04 s\r[                                        ] | 0% Completed | 136.14 s\r[                                        ] | 0% Completed | 136.24 s\r[                                        ] | 0% Completed | 136.35 s\r[                                        ] | 0% Completed | 136.45 s\r[                                        ] | 0% Completed | 136.56 s\r[                                        ] | 0% Completed | 136.66 s\r[                                        ] | 0% Completed | 136.76 s\r[                                        ] | 0% Completed | 136.87 s\r[                                        ] | 0% Completed | 136.98 s\r[                                        ] | 0% Completed | 137.08 s\r[                                        ] | 0% Completed | 137.18 s\r[                                        ] | 0% Completed | 137.29 s\r[                                        ] | 0% Completed | 137.39 s\r[                                        ] | 0% Completed | 137.49 s\r[                                        ] | 0% Completed | 137.59 s\r[                                        ] | 0% Completed | 137.70 s\r[                                        ] | 0% Completed | 137.80 s\r[                                        ] | 0% Completed | 137.90 s\r[                                        ] | 0% Completed | 138.01 s\r[                                        ] | 0% Completed | 138.11 s\r[                                        ] | 0% Completed | 138.21 s\r[                                        ] | 0% Completed | 138.32 s\r[                                        ] | 0% Completed | 138.42 s\r[                                        ] | 0% Completed | 138.52 s\r[                                        ] | 0% Completed | 138.63 s\r[                                        ] | 0% Completed | 138.73 s\r[                                        ] | 0% Completed | 138.83 s\r[                                        ] | 0% Completed | 138.94 s\r[####################                    ] | 50% Completed | 139.04 s\r[####################                    ] | 50% Completed | 139.14 s\r[####################                    ] | 50% Completed | 139.24 s\r[####################                    ] | 50% Completed | 139.34 s\r[####################                    ] | 50% Completed | 139.45 s\r[####################                    ] | 50% Completed | 139.55 s\r[####################                    ] | 50% Completed | 139.65 s\r[####################                    ] | 50% Completed | 139.75 s\r[####################                    ] | 50% Completed | 139.85 s\r[####################                    ] | 50% Completed | 139.95 s\r[####################                    ] | 50% Completed | 140.06 s\r[####################                    ] | 50% Completed | 140.16 s\r[####################                    ] | 50% Completed | 140.26 s\r[####################                    ] | 50% Completed | 140.36 s\r[####################                    ] | 50% Completed | 140.46 s\r[####################                    ] | 50% Completed | 140.57 s\r[####################                    ] | 50% Completed | 140.67 s\r[####################                    ] | 50% Completed | 140.77 s\r[####################                    ] | 50% Completed | 140.87 s\r[####################                    ] | 50% Completed | 140.97 s\r[####################                    ] | 50% Completed | 141.08 s\r[####################                    ] | 50% Completed | 141.18 s\r[####################                    ] | 50% Completed | 141.28 s\r[####################                    ] | 50% Completed | 141.38 s\r[####################                    ] | 50% Completed | 141.48 s\r[####################                    ] | 50% Completed | 141.58 s\r[####################                    ] | 50% Completed | 141.69 s\r[####################                    ] | 50% Completed | 141.79 s\r[####################                    ] | 50% Completed | 141.89 s\r[####################                    ] | 50% Completed | 141.99 s\r[####################                    ] | 50% Completed | 142.09 s\r[####################                    ] | 50% Completed | 142.20 s\r[####################                    ] | 50% Completed | 142.30 s\r[####################                    ] | 50% Completed | 142.40 s\r[####################                    ] | 50% Completed | 142.50 s\r[####################                    ] | 50% Completed | 142.61 s\r[####################                    ] | 50% Completed | 142.71 s\r[####################                    ] | 50% Completed | 142.81 s\r[####################                    ] | 50% Completed | 142.91 s\r[####################                    ] | 50% Completed | 143.01 s\r[####################                    ] | 50% Completed | 143.11 s\r[####################                    ] | 50% Completed | 143.22 s\r[####################                    ] | 50% Completed | 143.32 s\r[####################                    ] | 50% Completed | 143.42 s\r[####################                    ] | 50% Completed | 143.53 s\r[####################                    ] | 50% Completed | 143.63 s\r[####################                    ] | 50% Completed | 143.73 s\r[####################                    ] | 50% Completed | 143.83 s\r[####################                    ] | 50% Completed | 143.94 s\r[####################                    ] | 50% Completed | 144.04 s\r[####################                    ] | 50% Completed | 144.14 s\r[####################                    ] | 50% Completed | 144.24 s\r[####################                    ] | 50% Completed | 144.34 s\r[####################                    ] | 50% Completed | 144.44 s\r[####################                    ] | 50% Completed | 144.55 s\r[####################                    ] | 50% Completed | 144.65 s\r[####################                    ] | 50% Completed | 144.75 s\r[####################                    ] | 50% Completed | 144.85 s\r[####################                    ] | 50% Completed | 144.95 s\r[####################                    ] | 50% Completed | 145.05 s\r[####################                    ] | 50% Completed | 145.16 s\r[####################                    ] | 50% Completed | 145.26 s\r[####################                    ] | 50% Completed | 145.36 s\r[####################                    ] | 50% Completed | 145.46 s\r[####################                    ] | 50% Completed | 145.56 s\r[####################                    ] | 50% Completed | 145.67 s\r[####################                    ] | 50% Completed | 145.77 s\r[####################                    ] | 50% Completed | 145.87 s\r[####################                    ] | 50% Completed | 145.97 s\r[####################                    ] | 50% Completed | 146.07 s\r[####################                    ] | 50% Completed | 146.18 s\r[####################                    ] | 50% Completed | 146.28 s\r[####################                    ] | 50% Completed | 146.38 s\r[####################                    ] | 50% Completed | 146.48 s\r[####################                    ] | 50% Completed | 146.58 s\r[####################                    ] | 50% Completed | 146.69 s\r[####################                    ] | 50% Completed | 146.79 s\r[####################                    ] | 50% Completed | 146.89 s\r[####################                    ] | 50% Completed | 146.99 s\r[####################                    ] | 50% Completed | 147.09 s\r[####################                    ] | 50% Completed | 147.20 s\r[####################                    ] | 50% Completed | 147.30 s\r[####################                    ] | 50% Completed | 147.40 s\r[####################                    ] | 50% Completed | 147.50 s\r[####################                    ] | 50% Completed | 147.60 s\r[####################                    ] | 50% Completed | 147.70 s\r[####################                    ] | 50% Completed | 147.81 s\r[####################                    ] | 50% Completed | 147.91 s\r[####################                    ] | 50% Completed | 148.01 s\r[####################                    ] | 50% Completed | 148.11 s\r[####################                    ] | 50% Completed | 148.21 s\r[####################                    ] | 50% Completed | 148.31 s\r[####################                    ] | 50% Completed | 148.42 s\r[####################                    ] | 50% Completed | 148.52 s\r[####################                    ] | 50% Completed | 148.62 s\r[####################                    ] | 50% Completed | 148.72 s\r[####################                    ] | 50% Completed | 148.82 s\r[####################                    ] | 50% Completed | 148.92 s\r[####################                    ] | 50% Completed | 149.03 s\r[####################                    ] | 50% Completed | 149.13 s\r[####################                    ] | 50% Completed | 149.23 s\r[####################                    ] | 50% Completed | 149.33 s\r[####################                    ] | 50% Completed | 149.43 s\r[####################                    ] | 50% Completed | 149.54 s\r[####################                    ] | 50% Completed | 149.64 s\r[####################                    ] | 50% Completed | 149.74 s\r[####################                    ] | 50% Completed | 149.84 s\r[####################                    ] | 50% Completed | 149.94 s\r[####################                    ] | 50% Completed | 150.04 s\r[####################                    ] | 50% Completed | 150.15 s\r[####################                    ] | 50% Completed | 150.25 s\r[####################                    ] | 50% Completed | 150.35 s\r[####################                    ] | 50% Completed | 150.45 s\r[####################                    ] | 50% Completed | 150.55 s\r[####################                    ] | 50% Completed | 150.65 s\r[####################                    ] | 50% Completed | 150.76 s\r[####################                    ] | 50% Completed | 150.86 s\r[####################                    ] | 50% Completed | 150.96 s\r[####################                    ] | 50% Completed | 151.06 s\r[####################                    ] | 50% Completed | 151.16 s\r[####################                    ] | 50% Completed | 151.26 s\r[####################                    ] | 50% Completed | 151.36 s\r[####################                    ] | 50% Completed | 151.47 s\r[####################                    ] | 50% Completed | 151.57 s\r[####################                    ] | 50% Completed | 151.67 s\r[####################                    ] | 50% Completed | 151.77 s\r[####################                    ] | 50% Completed | 151.87 s\r[####################                    ] | 50% Completed | 151.97 s\r[####################                    ] | 50% Completed | 152.08 s\r[####################                    ] | 50% Completed | 152.18 s\r[####################                    ] | 50% Completed | 152.28 s\r[####################                    ] | 50% Completed | 152.38 s\r[####################                    ] | 50% Completed | 152.48 s\r[####################                    ] | 50% Completed | 152.59 s\r[####################                    ] | 50% Completed | 152.69 s\r[####################                    ] | 50% Completed | 152.79 s\r[####################                    ] | 50% Completed | 152.89 s\r[####################                    ] | 50% Completed | 152.99 s\r[####################                    ] | 50% Completed | 153.10 s\r[####################                    ] | 50% Completed | 153.20 s\r[####################                    ] | 50% Completed | 153.30 s\r[####################                    ] | 50% Completed | 153.40 s\r[####################                    ] | 50% Completed | 153.50 s\r[####################                    ] | 50% Completed | 153.61 s\r[####################                    ] | 50% Completed | 153.71 s\r[####################                    ] | 50% Completed | 153.81 s\r[####################                    ] | 50% Completed | 153.91 s\r[####################                    ] | 50% Completed | 154.01 s\r[####################                    ] | 50% Completed | 154.11 s\r[####################                    ] | 50% Completed | 154.22 s\r[####################                    ] | 50% Completed | 154.32 s\r[####################                    ] | 50% Completed | 154.42 s\r[####################                    ] | 50% Completed | 154.52 s\r[####################                    ] | 50% Completed | 154.62 s\r[####################                    ] | 50% Completed | 154.72 s\r[####################                    ] | 50% Completed | 154.83 s\r[####################                    ] | 50% Completed | 154.93 s\r[####################                    ] | 50% Completed | 155.03 s\r[####################                    ] | 50% Completed | 155.13 s\r[####################                    ] | 50% Completed | 155.24 s\r[####################                    ] | 50% Completed | 155.34 s\r[####################                    ] | 50% Completed | 155.44 s\r[####################                    ] | 50% Completed | 155.54 s\r[####################                    ] | 50% Completed | 155.64 s\r[####################                    ] | 50% Completed | 155.75 s\r[####################                    ] | 50% Completed | 155.85 s\r[####################                    ] | 50% Completed | 155.95 s\r[####################                    ] | 50% Completed | 156.05 s\r[####################                    ] | 50% Completed | 156.15 s\r[####################                    ] | 50% Completed | 156.25 s\r[####################                    ] | 50% Completed | 156.36 s\r[####################                    ] | 50% Completed | 156.46 s\r[####################                    ] | 50% Completed | 156.56 s\r[####################                    ] | 50% Completed | 156.66 s\r[####################                    ] | 50% Completed | 156.77 s\r[####################                    ] | 50% Completed | 156.89 s\r[####################                    ] | 50% Completed | 156.99 s\r[####################                    ] | 50% Completed | 157.09 s\r[####################                    ] | 50% Completed | 157.20 s\r[####################                    ] | 50% Completed | 157.30 s\r[####################                    ] | 50% Completed | 157.40 s\r[####################                    ] | 50% Completed | 157.50 s\r[####################                    ] | 50% Completed | 157.60 s\r[####################                    ] | 50% Completed | 157.70 s\r[####################                    ] | 50% Completed | 157.80 s\r[####################                    ] | 50% Completed | 157.91 s\r[####################                    ] | 50% Completed | 158.01 s\r[####################                    ] | 50% Completed | 158.11 s\r[####################                    ] | 50% Completed | 158.21 s\r[####################                    ] | 50% Completed | 158.31 s\r[####################                    ] | 50% Completed | 158.42 s\r[####################                    ] | 50% Completed | 158.52 s\r[####################                    ] | 50% Completed | 158.62 s\r[####################                    ] | 50% Completed | 158.72 s\r[####################                    ] | 50% Completed | 158.82 s\r[####################                    ] | 50% Completed | 158.93 s\r[####################                    ] | 50% Completed | 159.05 s\r[####################                    ] | 50% Completed | 159.15 s\r[####################                    ] | 50% Completed | 159.25 s\r[####################                    ] | 50% Completed | 159.35 s\r[####################                    ] | 50% Completed | 159.46 s\r[####################                    ] | 50% Completed | 159.56 s\r[####################                    ] | 50% Completed | 159.66 s\r[####################                    ] | 50% Completed | 159.76 s\r[####################                    ] | 50% Completed | 159.87 s\r[####################                    ] | 50% Completed | 159.97 s\r[####################                    ] | 50% Completed | 160.07 s\r[####################                    ] | 50% Completed | 160.17 s\r[####################                    ] | 50% Completed | 160.28 s\r[####################                    ] | 50% Completed | 160.38 s\r[####################                    ] | 50% Completed | 160.48 s\r[####################                    ] | 50% Completed | 160.58 s\r[####################                    ] | 50% Completed | 160.68 s\r[####################                    ] | 50% Completed | 160.79 s\r[####################                    ] | 50% Completed | 160.89 s\r[####################                    ] | 50% Completed | 160.99 s\r[####################                    ] | 50% Completed | 161.09 s\r[########################################] | 100% Completed | 161.20 s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n/root/.ipykernel/7698/command-4106393328968512-2900383764:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n"
     ]
    }
   ],
   "source": [
    "currdf_old = dd.from_pandas(currdf_old.head(2), npartitions = 4)\n",
    "for label, section in {'FILT_MD': 'FILT_MD', 'FILT_QA': 'FILT_QA'}.items():\n",
    "\n",
    "  currdf_old['matches_' + label] = currdf_old[section].apply(lambda x: match_count_lowStat(x, word_set_dict_old, phrases = True, suppress = negate_dict1_old), meta = ('matches_' + label, object))\n",
    "  #currdf_old[label] = currdf_old['matches_' + label].apply(lambda x: [str(calc['filt']) for calc in x], meta = ('FILT_' + label, object))\n",
    "\n",
    "# Running Dask compute\n",
    "with ProgressBar():\n",
    "  currdf_old = currdf_old.compute()\n",
    "\n",
    "\n",
    "for label, section in {'FILT_MD': 'MGNT_DISCUSSION', 'FILT_QA': 'QA_SECTION'}.items():\n",
    "  currdf_old['matches_' + label] = currdf_old['matches_' + label].apply(ast.literal_eval)\n",
    "  for topic in word_set_dict_old.keys():\n",
    "    \n",
    "    currdf_old[topic + '_TOTAL_' + label] = currdf_old['matches_' + label].apply(lambda x: x[topic]['total'])\n",
    "    currdf_old[topic + '_STATS_' + label] = currdf_old['matches_' + label].apply(lambda x: x[topic]['stats'])\n",
    "\n",
    "  currdf_old.drop(['matches_' + label], axis = 1, inplace = True)\n",
    "  gc.collect()\n",
    "  \n",
    "# Calculate additional stats derived from count stats & sentiment\n",
    "\n",
    "for label, section in {'FILT_MD': 'MGNT_DISCUSSION', 'FILT_QA': 'QA_SECTION'}.items():\n",
    "  \n",
    " \n",
    "  for topic in word_set_dict_old.keys():\n",
    "  \n",
    "    # relevance = #sentences detected with topic / #total sentences\n",
    "    currdf_old[topic + '_REL_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0])/len(x) if len(x)>0 else None)\n",
    "    currdf_old[topic + '_COUNT_' + label] = currdf_old[topic + '_TOTAL_' + label].apply(lambda x: len([a for a in x if a>0]) if len(x)>0 else None)\n",
    "    currdf_old[topic + '_SENT_' + label] = currdf_old[[topic + '_TOTAL_' + label, 'SENT_LABELS_' + label]].apply(lambda x: sentscore(x[0], x[1], weight = False), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ca6131a-5982-484e-8422-98627f4c4cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c3635c-fa60-46ea-960f-c68ef27fb528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:04:52\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:228\u001B[0m | Applying initial match count transformations.\n\u001B[32m2024-12-13 15:04:52\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'FILT_MD' to create 'matches_FILT_MD'.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'matches_FILT_MD'.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'FILT_QA' to create 'matches_FILT_QA'.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'matches_FILT_QA'.\n\r[                                        ] | 0% Completed | 415.79 us\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\r[###########                             ] | 28% Completed | 104.51 ms\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:53\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\r[###########                             ] | 28% Completed | 207.31 ms\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\r[###########                             ] | 28% Completed | 309.53 ms\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\r[###########                             ] | 28% Completed | 411.65 ms\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\r[###########                             ] | 28% Completed | 514.32 ms\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\r[###########                             ] | 28% Completed | 616.20 ms\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\r[###########                             ] | 28% Completed | 717.83 ms\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\r[###########                             ] | 28% Completed | 819.90 ms\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\r[###########                             ] | 28% Completed | 923.13 ms\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\r[###########                             ] | 28% Completed | 1.03 s\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\r[###########                             ] | 28% Completed | 1.13 s\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:54\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:55\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:55\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:55\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:55\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:55\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 0 tokens.\n\u001B[32m2024-12-13 15:04:55\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:55\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:04:55\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mtext_preprocessing.py:197\u001B[0m | Tokenized document into 1 tokens.\n\u001B[32m2024-12-13 15:\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n0m | Applying transformation on single column 'matches_FILT_MD' to create 'SUPPLY_CHAIN_SHORTAGE_STATS_FILT_MD'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SUPPLY_CHAIN_SHORTAGE_STATS_FILT_MD'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'SUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_MD' to create 'SUPPLY_CHAIN_SHORTAGE_REL_FILT_MD'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SUPPLY_CHAIN_SHORTAGE_REL_FILT_MD'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'SUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_MD' to create 'SUPPLY_CHAIN_SHORTAGE_COUNT_FILT_MD'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SUPPLY_CHAIN_SHORTAGE_COUNT_FILT_MD'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['SUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_MD', 'SENT_LABELS_FILT_MD'] to create 'SUPPLY_CHAIN_SHORTAGE_SENT_FILT_MD'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SUPPLY_CHAIN_SHORTAGE_SENT_FILT_MD'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'JOB_CREATION' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'JOB_CREATION_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'JOB_CREATION_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'JOB_CREATION_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'JOB_CREATION_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'JOB_CREATION_TOTAL_FILT_QA' to create 'JOB_CREATION_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'JOB_CREATION_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:41\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'JOB_CREATION_TOTAL_FILT_QA' to create 'JOB_CREATION_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'JOB_CREATION_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['JOB_CREATION_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'JOB_CREATION_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'JOB_CREATION_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'JOB_REDUCTION' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'JOB_REDUCTION_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'JOB_REDUCTION_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'JOB_REDUCTION_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'JOB_REDUCTION_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'JOB_REDUCTION_TOTAL_FILT_QA' to create 'JOB_REDUCTION_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'JOB_REDUCTION_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'JOB_REDUCTION_TOTAL_FILT_QA' to create 'JOB_REDUCTION_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'JOB_REDUCTION_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['JOB_REDUCTION_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'JOB_REDUCTION_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'JOB_REDUCTION_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'LABOR_SHORTAGE' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'LABOR_SHORTAGE_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LABOR_SHORTAGE_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'LABOR_SHORTAGE_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LABOR_SHORTAGE_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'LABOR_SHORTAGE_TOTAL_FILT_QA' to create 'LABOR_SHORTAGE_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LABOR_SHORTAGE_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'LABOR_SHORTAGE_TOTAL_FILT_QA' to create 'LABOR_SHORTAGE_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LABOR_SHORTAGE_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['LABOR_SHORTAGE_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'LABOR_SHORTAGE_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LABOR_SHORTAGE_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'AUTOMATION' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'AUTOMATION_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'AUTOMATION_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'AUTOMATION_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'AUTOMATION_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'AUTOMATION_TOTAL_FILT_QA' to create 'AUTOMATION_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'AUTOMATION_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'AUTOMATION_TOTAL_FILT_QA' to create 'AUTOMATION_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'AUTOMATION_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['AUTOMATION_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'AUTOMATION_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'AUTOMATION_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'EDUCATION_TRAINING_PROGRAM' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'EDUCATION_TRAINING_PROGRAM_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'EDUCATION_TRAINING_PROGRAM_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'EDUCATION_TRAINING_PROGRAM_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'EDUCATION_TRAINING_PROGRAM_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'EDUCATION_TRAINING_PROGRAM_TOTAL_FILT_QA' to create 'EDUCATION_TRAINING_PROGRAM_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'EDUCATION_TRAINING_PROGRAM_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'EDUCATION_TRAINING_PROGRAM_TOTAL_FILT_QA' to create 'EDUCATION_TRAINING_PROGRAM_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'EDUCATION_TRAINING_PROGRAM_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['EDUCATION_TRAINING_PROGRAM_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'EDUCATION_TRAINING_PROGRAM_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'EDUCATION_TRAINING_PROGRAM_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'WAGE_INCREASES' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'WAGE_INCREASES_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'WAGE_INCREASES_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'WAGE_INCREASES_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'WAGE_INCREASES_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'WAGE_INCREASES_TOTAL_FILT_QA' to create 'WAGE_INCREASES_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'WAGE_INCREASES_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'WAGE_INCREASES_TOTAL_FILT_QA' to create 'WAGE_INCREASES_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'WAGE_INCREASES_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['WAGE_INCREASES_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'WAGE_INCREASES_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'WAGE_INCREASES_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'CHALLEGING_UNCERTAIN_VOLATILE' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'CHALLEGING_UNCERTAIN_VOLATILE_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'CHALLEGING_UNCERTAIN_VOLATILE_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'CHALLEGING_UNCERTAIN_VOLATILE_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'CHALLEGING_UNCERTAIN_VOLATILE_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'CHALLEGING_UNCERTAIN_VOLATILE_TOTAL_FILT_QA' to create 'CHALLEGING_UNCERTAIN_VOLATILE_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'CHALLEGING_UNCERTAIN_VOLATILE_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'CHALLEGING_UNCERTAIN_VOLATILE_TOTAL_FILT_QA' to create 'CHALLEGING_UNCERTAIN_VOLATILE_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'CHALLEGING_UNCERTAIN_VOLATILE_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['CHALLEGING_UNCERTAIN_VOLATILE_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'CHALLEGING_UNCERTAIN_VOLATILE_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'CHALLEGING_UNCERTAIN_VOLATILE_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'COSTCUTTING' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'COSTCUTTING_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'COSTCUTTING_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'COSTCUTTING_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'COSTCUTTING_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'COSTCUTTING_TOTAL_FILT_QA' to create 'COSTCUTTING_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'COSTCUTTING_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'COSTCUTTING_TOTAL_FILT_QA' to create 'COSTCUTTING_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'COSTCUTTING_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['COSTCUTTING_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'COSTCUTTING_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'COSTCUTTING_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'DISRUPTION' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'DISRUPTION_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'DISRUPTION_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'DISRUPTION_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'DISRUPTION_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'DISRUPTION_TOTAL_FILT_QA' to create 'DISRUPTION_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'DISRUPTION_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'DISRUPTION_TOTAL_FILT_QA' to create 'DISRUPTION_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'DISRUPTION_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['DISRUPTION_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'DISRUPTION_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'DISRUPTION_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'INFLATION' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'INFLATION_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'INFLATION_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'INFLATION_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'INFLATION_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'INFLATION_TOTAL_FILT_QA' to create 'INFLATION_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'INFLATION_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'INFLATION_TOTAL_FILT_QA' to create 'INFLATION_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'INFLATION_COUNT_FILT_QA'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['INFLATION_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'INFLATION_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'INFLATION_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'RESTOCKING' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'RESTOCKING_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'RESTOCKING_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'RESTOCKING_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'RESTOCKING_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'RESTOCKING_TOTAL_FILT_QA' to create 'RESTOCKING_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'RESTOCKING_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'RESTOCKING_TOTAL_FILT_QA' to create 'RESTOCKING_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'RESTOCKING_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['RESTOCKING_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'RESTOCKING_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'RESTOCKING_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'LIQUIDITY' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'LIQUIDITY_TOTAL_FILT_QA'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LIQUIDITY_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'LIQUIDITY_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LIQUIDITY_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'LIQUIDITY_TOTAL_FILT_QA' to create 'LIQUIDITY_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LIQUIDITY_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'LIQUIDITY_TOTAL_FILT_QA' to create 'LIQUIDITY_COUNT_FILT_QA'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LIQUIDITY_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['LIQUIDITY_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'LIQUIDITY_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'LIQUIDITY_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'GEOPOL_UNCERTAINTY' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'GEOPOL_UNCERTAINTY_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'GEOPOL_UNCERTAINTY_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'GEOPOL_UNCERTAINTY_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'GEOPOL_UNCERTAINTY_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'GEOPOL_UNCERTAINTY_TOTAL_FILT_QA' to create 'GEOPOL_UNCERTAINTY_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'GEOPOL_UNCERTAINTY_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'GEOPOL_UNCERTAINTY_TOTAL_FILT_QA' to create 'GEOPOL_UNCERTAINTY_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'GEOPOL_UNCERTAINTY_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['GEOPOL_UNCERTAINTY_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'GEOPOL_UNCERTAINTY_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'GEOPOL_UNCERTAINTY_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'PROBLEM' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'PROBLEM_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'PROBLEM_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'PROBLEM_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'PROBLEM_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'PROBLEM_TOTAL_FILT_QA' to create 'PROBLEM_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'PROBLEM_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'PROBLEM_TOTAL_FILT_QA' to create 'PROBLEM_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'PROBLEM_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['PROBLEM_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'PROBLEM_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'PROBLEM_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'RECESSION_MELTDOWN_BLACK_SWAN' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'RECESSION_MELTDOWN_BLACK_SWAN_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'RECESSION_MELTDOWN_BLACK_SWAN_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'RECESSION_MELTDOWN_BLACK_SWAN_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'RECESSION_MELTDOWN_BLACK_SWAN_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'RECESSION_MELTDOWN_BLACK_SWAN_TOTAL_FILT_QA' to create 'RECESSION_MELTDOWN_BLACK_SWAN_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'RECESSION_MELTDOWN_BLACK_SWAN_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'RECESSION_MELTDOWN_BLACK_SWAN_TOTAL_FILT_QA' to create 'RECESSION_MELTDOWN_BLACK_SWAN_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'RECESSION_MELTDOWN_BLACK_SWAN_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['RECESSION_MELTDOWN_BLACK_SWAN_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'RECESSION_MELTDOWN_BLACK_SWAN_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df[columns_to_use].apply(func)\n/Workspace/Users/santhosh.kumar3@voya.com/MLFlow_and_NLI_finetune/centralized_nlp_package/data_processing/dataframe_utils.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[new_column] = df.apply(lambda row: func(row), axis=1)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'RECESSION_MELTDOWN_BLACK_SWAN_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'ECONOMIC_DOWNTURN' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'ECONOMIC_DOWNTURN_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'ECONOMIC_DOWNTURN_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'ECONOMIC_DOWNTURN_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'ECONOMIC_DOWNTURN_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'ECONOMIC_DOWNTURN_TOTAL_FILT_QA' to create 'ECONOMIC_DOWNTURN_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'ECONOMIC_DOWNTURN_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'ECONOMIC_DOWNTURN_TOTAL_FILT_QA' to create 'ECONOMIC_DOWNTURN_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'ECONOMIC_DOWNTURN_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['ECONOMIC_DOWNTURN_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'ECONOMIC_DOWNTURN_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'ECONOMIC_DOWNTURN_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_TOTAL_FILT_QA' to create 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_TOTAL_FILT_QA' to create 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'BOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:246\u001B[0m | Applying transformations for topic 'SUPPLY_CHAIN_SHORTAGE' and label 'FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'SUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'matches_FILT_QA' to create 'SUPPLY_CHAIN_SHORTAGE_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SUPPLY_CHAIN_SHORTAGE_STATS_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'SUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_QA' to create 'SUPPLY_CHAIN_SHORTAGE_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SUPPLY_CHAIN_SHORTAGE_REL_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:111\u001B[0m | Applying transformation on single column 'SUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_QA' to create 'SUPPLY_CHAIN_SHORTAGE_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SUPPLY_CHAIN_SHORTAGE_COUNT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:118\u001B[0m | Applying transformation on multiple columns ['SUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_QA', 'SENT_LABELS_FILT_QA'] to create 'SUPPLY_CHAIN_SHORTAGE_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[33m\u001B[1mWARNING\u001B[0m | \u001B[36mtext_analysis.py:695\u001B[0m | Indicators and weights must be of the same non-zero length.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[34m\u001B[1mDEBUG  \u001B[0m | \u001B[36mdataframe_utils.py:127\u001B[0m | Successfully applied transformation for 'SUPPLY_CHAIN_SHORTAGE_SENT_FILT_QA'.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mdataframe_utils.py:132\u001B[0m | All transformations applied successfully.\n\u001B[32m2024-12-13 15:08:42\u001B[0m | \u001B[1mINFO   \u001B[0m | \u001B[36mreport_generation.py:252\u001B[0m | Dropped intermediate match columns: ['matches_FILT_MD', 'matches_FILT_QA']\n"
     ]
    }
   ],
   "source": [
    "currdf_ref = generate_topic_report(currdf_ref.head(2), word_set_dict_ref, negate_dict1_ref,nlp_ref, stats_list = ['total', 'stats','relevance', 'count', 'sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c106b3-67e3-488d-ad3b-1588248fa7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert currdf_old.equals(currdf_ref[currdf_old.columns]), \"The DataFrames are not identical.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d828a6c7-a6a3-4773-a8db-42ae545c2951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Writing Back to Snowflake table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00771313-f1ab-413f-8bbb-536359833e41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cf7b8b-8f52-4128-8f12-4ad5acb00981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Auxiliar functions\n",
    "def equivalent_type(string, f):\n",
    "    print(string, f)\n",
    "   # if 'DATE' == string: return StringType()\n",
    "    \n",
    "    if f == 'datetime64[ns]': return TimestampType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return FloatType()\n",
    "    elif 'FILT_MD' == string: return ArrayType(StringType())\n",
    "    elif 'FILT_QA' == string: return ArrayType(StringType())\n",
    "    elif '_len_' in string.lower(): return ArrayType(IntegerType())\n",
    "    elif '_total_' in string.lower(): return ArrayType(IntegerType())\n",
    "    elif '_count_' in string.lower(): return IntegerType()\n",
    "    elif '_stats_' in string.lower(): return MapType(StringType(), IntegerType())\n",
    "    elif 'sent_scores' in string.lower(): return ArrayType(FloatType())\n",
    "    elif 'sent_labels' in string.lower(): return ArrayType(IntegerType())\n",
    " #   elif f == 'object': return ArrayType()\n",
    " #   elif f == 'list': return ArrayType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, format_type):\n",
    "    #try: \n",
    "    typo = equivalent_type(string, format_type)\n",
    "    print(typo)\n",
    "    #except: typo = StringType()\n",
    "    return StructField(string, typo)\n",
    "\n",
    "# Given pandas dataframe, it will return a spark's dataframe.\n",
    "def pandas_to_spark_old(pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "      struct_list.append(define_structure(column, typo))\n",
    "      print(column)\n",
    "    p_schema = StructType(struct_list)\n",
    "    return spark.createDataFrame(pandas_df, p_schema)                                                                        \n",
    "# new_sf.db = 'EDS_PROD'\n",
    "# new_sf.schema = 'QUANT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89a5269-7a62-45b6-8100-144e1c70fd7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALL_ID string\nStringType()\nCALL_ID\nENTITY_ID string\nStringType()\nENTITY_ID\nDATE string\nStringType()\nDATE\nFILT_MD string\nArrayType(StringType(), True)\nFILT_MD\nFILT_QA string\nArrayType(StringType(), True)\nFILT_QA\nSENT_LABELS_FILT_MD object\nArrayType(IntegerType(), True)\nSENT_LABELS_FILT_MD\nSENT_LABELS_FILT_QA object\nArrayType(IntegerType(), True)\nSENT_LABELS_FILT_QA\nCALL_NAME string\nStringType()\nCALL_NAME\nCOMPANY_NAME string\nStringType()\nCOMPANY_NAME\nEARNINGS_CALL string\nStringType()\nEARNINGS_CALL\nERROR string\nStringType()\nERROR\nTRANSCRIPT_STATUS string\nStringType()\nTRANSCRIPT_STATUS\nUPLOAD_DT_UTC datetime64[us]\nStringType()\nUPLOAD_DT_UTC\nVERSION_ID string\nStringType()\nVERSION_ID\nEVENT_DATETIME_UTC datetime64[us]\nStringType()\nEVENT_DATETIME_UTC\nPARSED_DATETIME_EASTERN_TZ datetime64[us]\nStringType()\nPARSED_DATETIME_EASTERN_TZ\nLEN_FILT_MD int64\nLongType()\nLEN_FILT_MD\nLEN_FILT_QA int64\nLongType()\nLEN_FILT_QA\nJOB_CREATION_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nJOB_CREATION_TOTAL_FILT_MD\nJOB_CREATION_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nJOB_CREATION_STATS_FILT_MD\nJOB_REDUCTION_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nJOB_REDUCTION_TOTAL_FILT_MD\nJOB_REDUCTION_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nJOB_REDUCTION_STATS_FILT_MD\nLABOR_SHORTAGE_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nLABOR_SHORTAGE_TOTAL_FILT_MD\nLABOR_SHORTAGE_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nLABOR_SHORTAGE_STATS_FILT_MD\nAUTOMATION_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nAUTOMATION_TOTAL_FILT_MD\nAUTOMATION_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nAUTOMATION_STATS_FILT_MD\nEDUCATION_TRAINING_PROGRAM_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nEDUCATION_TRAINING_PROGRAM_TOTAL_FILT_MD\nEDUCATION_TRAINING_PROGRAM_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nEDUCATION_TRAINING_PROGRAM_STATS_FILT_MD\nWAGE_INCREASES_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nWAGE_INCREASES_TOTAL_FILT_MD\nWAGE_INCREASES_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nWAGE_INCREASES_STATS_FILT_MD\nCHALLEGING_UNCERTAIN_VOLATILE_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nCHALLEGING_UNCERTAIN_VOLATILE_TOTAL_FILT_MD\nCHALLEGING_UNCERTAIN_VOLATILE_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nCHALLEGING_UNCERTAIN_VOLATILE_STATS_FILT_MD\nCOSTCUTTING_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nCOSTCUTTING_TOTAL_FILT_MD\nCOSTCUTTING_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nCOSTCUTTING_STATS_FILT_MD\nDISRUPTION_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nDISRUPTION_TOTAL_FILT_MD\nDISRUPTION_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nDISRUPTION_STATS_FILT_MD\nINFLATION_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nINFLATION_TOTAL_FILT_MD\nINFLATION_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nINFLATION_STATS_FILT_MD\nRESTOCKING_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nRESTOCKING_TOTAL_FILT_MD\nRESTOCKING_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nRESTOCKING_STATS_FILT_MD\nLIQUIDITY_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nLIQUIDITY_TOTAL_FILT_MD\nLIQUIDITY_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nLIQUIDITY_STATS_FILT_MD\nGEOPOL_UNCERTAINTY_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nGEOPOL_UNCERTAINTY_TOTAL_FILT_MD\nGEOPOL_UNCERTAINTY_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nGEOPOL_UNCERTAINTY_STATS_FILT_MD\nPROBLEM_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nPROBLEM_TOTAL_FILT_MD\nPROBLEM_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nPROBLEM_STATS_FILT_MD\nRECESSION_MELTDOWN_BLACK_SWAN_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nRECESSION_MELTDOWN_BLACK_SWAN_TOTAL_FILT_MD\nRECESSION_MELTDOWN_BLACK_SWAN_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nRECESSION_MELTDOWN_BLACK_SWAN_STATS_FILT_MD\nECONOMIC_DOWNTURN_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nECONOMIC_DOWNTURN_TOTAL_FILT_MD\nECONOMIC_DOWNTURN_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nECONOMIC_DOWNTURN_STATS_FILT_MD\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_TOTAL_FILT_MD\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_STATS_FILT_MD\nSUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_MD object\nArrayType(IntegerType(), True)\nSUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_MD\nSUPPLY_CHAIN_SHORTAGE_STATS_FILT_MD object\nMapType(StringType(), IntegerType(), True)\nSUPPLY_CHAIN_SHORTAGE_STATS_FILT_MD\nJOB_CREATION_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nJOB_CREATION_TOTAL_FILT_QA\nJOB_CREATION_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nJOB_CREATION_STATS_FILT_QA\nJOB_REDUCTION_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nJOB_REDUCTION_TOTAL_FILT_QA\nJOB_REDUCTION_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nJOB_REDUCTION_STATS_FILT_QA\nLABOR_SHORTAGE_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nLABOR_SHORTAGE_TOTAL_FILT_QA\nLABOR_SHORTAGE_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nLABOR_SHORTAGE_STATS_FILT_QA\nAUTOMATION_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nAUTOMATION_TOTAL_FILT_QA\nAUTOMATION_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nAUTOMATION_STATS_FILT_QA\nEDUCATION_TRAINING_PROGRAM_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nEDUCATION_TRAINING_PROGRAM_TOTAL_FILT_QA\nEDUCATION_TRAINING_PROGRAM_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nEDUCATION_TRAINING_PROGRAM_STATS_FILT_QA\nWAGE_INCREASES_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nWAGE_INCREASES_TOTAL_FILT_QA\nWAGE_INCREASES_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nWAGE_INCREASES_STATS_FILT_QA\nCHALLEGING_UNCERTAIN_VOLATILE_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nCHALLEGING_UNCERTAIN_VOLATILE_TOTAL_FILT_QA\nCHALLEGING_UNCERTAIN_VOLATILE_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nCHALLEGING_UNCERTAIN_VOLATILE_STATS_FILT_QA\nCOSTCUTTING_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nCOSTCUTTING_TOTAL_FILT_QA\nCOSTCUTTING_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nCOSTCUTTING_STATS_FILT_QA\nDISRUPTION_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nDISRUPTION_TOTAL_FILT_QA\nDISRUPTION_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nDISRUPTION_STATS_FILT_QA\nINFLATION_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nINFLATION_TOTAL_FILT_QA\nINFLATION_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nINFLATION_STATS_FILT_QA\nRESTOCKING_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nRESTOCKING_TOTAL_FILT_QA\nRESTOCKING_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nRESTOCKING_STATS_FILT_QA\nLIQUIDITY_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nLIQUIDITY_TOTAL_FILT_QA\nLIQUIDITY_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nLIQUIDITY_STATS_FILT_QA\nGEOPOL_UNCERTAINTY_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nGEOPOL_UNCERTAINTY_TOTAL_FILT_QA\nGEOPOL_UNCERTAINTY_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nGEOPOL_UNCERTAINTY_STATS_FILT_QA\nPROBLEM_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nPROBLEM_TOTAL_FILT_QA\nPROBLEM_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nPROBLEM_STATS_FILT_QA\nRECESSION_MELTDOWN_BLACK_SWAN_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nRECESSION_MELTDOWN_BLACK_SWAN_TOTAL_FILT_QA\nRECESSION_MELTDOWN_BLACK_SWAN_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nRECESSION_MELTDOWN_BLACK_SWAN_STATS_FILT_QA\nECONOMIC_DOWNTURN_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nECONOMIC_DOWNTURN_TOTAL_FILT_QA\nECONOMIC_DOWNTURN_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nECONOMIC_DOWNTURN_STATS_FILT_QA\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_TOTAL_FILT_QA\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_STATS_FILT_QA\nSUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_QA object\nArrayType(IntegerType(), True)\nSUPPLY_CHAIN_SHORTAGE_TOTAL_FILT_QA\nSUPPLY_CHAIN_SHORTAGE_STATS_FILT_QA object\nMapType(StringType(), IntegerType(), True)\nSUPPLY_CHAIN_SHORTAGE_STATS_FILT_QA\nJOB_CREATION_REL_FILT_MD float64\nFloatType()\nJOB_CREATION_REL_FILT_MD\nJOB_CREATION_COUNT_FILT_MD int64\nLongType()\nJOB_CREATION_COUNT_FILT_MD\nJOB_CREATION_SENT_FILT_MD object\nStringType()\nJOB_CREATION_SENT_FILT_MD\nJOB_REDUCTION_REL_FILT_MD float64\nFloatType()\nJOB_REDUCTION_REL_FILT_MD\nJOB_REDUCTION_COUNT_FILT_MD int64\nLongType()\nJOB_REDUCTION_COUNT_FILT_MD\nJOB_REDUCTION_SENT_FILT_MD object\nStringType()\nJOB_REDUCTION_SENT_FILT_MD\nLABOR_SHORTAGE_REL_FILT_MD float64\nFloatType()\nLABOR_SHORTAGE_REL_FILT_MD\nLABOR_SHORTAGE_COUNT_FILT_MD int64\nLongType()\nLABOR_SHORTAGE_COUNT_FILT_MD\nLABOR_SHORTAGE_SENT_FILT_MD object\nStringType()\nLABOR_SHORTAGE_SENT_FILT_MD\nAUTOMATION_REL_FILT_MD float64\nFloatType()\nAUTOMATION_REL_FILT_MD\nAUTOMATION_COUNT_FILT_MD int64\nLongType()\nAUTOMATION_COUNT_FILT_MD\nAUTOMATION_SENT_FILT_MD object\nStringType()\nAUTOMATION_SENT_FILT_MD\nEDUCATION_TRAINING_PROGRAM_REL_FILT_MD float64\nFloatType()\nEDUCATION_TRAINING_PROGRAM_REL_FILT_MD\nEDUCATION_TRAINING_PROGRAM_COUNT_FILT_MD int64\nLongType()\nEDUCATION_TRAINING_PROGRAM_COUNT_FILT_MD\nEDUCATION_TRAINING_PROGRAM_SENT_FILT_MD object\nStringType()\nEDUCATION_TRAINING_PROGRAM_SENT_FILT_MD\nWAGE_INCREASES_REL_FILT_MD float64\nFloatType()\nWAGE_INCREASES_REL_FILT_MD\nWAGE_INCREASES_COUNT_FILT_MD int64\nLongType()\nWAGE_INCREASES_COUNT_FILT_MD\nWAGE_INCREASES_SENT_FILT_MD object\nStringType()\nWAGE_INCREASES_SENT_FILT_MD\nCHALLEGING_UNCERTAIN_VOLATILE_REL_FILT_MD float64\nFloatType()\nCHALLEGING_UNCERTAIN_VOLATILE_REL_FILT_MD\nCHALLEGING_UNCERTAIN_VOLATILE_COUNT_FILT_MD int64\nLongType()\nCHALLEGING_UNCERTAIN_VOLATILE_COUNT_FILT_MD\nCHALLEGING_UNCERTAIN_VOLATILE_SENT_FILT_MD object\nStringType()\nCHALLEGING_UNCERTAIN_VOLATILE_SENT_FILT_MD\nCOSTCUTTING_REL_FILT_MD float64\nFloatType()\nCOSTCUTTING_REL_FILT_MD\nCOSTCUTTING_COUNT_FILT_MD int64\nLongType()\nCOSTCUTTING_COUNT_FILT_MD\nCOSTCUTTING_SENT_FILT_MD object\nStringType()\nCOSTCUTTING_SENT_FILT_MD\nDISRUPTION_REL_FILT_MD float64\nFloatType()\nDISRUPTION_REL_FILT_MD\nDISRUPTION_COUNT_FILT_MD int64\nLongType()\nDISRUPTION_COUNT_FILT_MD\nDISRUPTION_SENT_FILT_MD object\nStringType()\nDISRUPTION_SENT_FILT_MD\nINFLATION_REL_FILT_MD float64\nFloatType()\nINFLATION_REL_FILT_MD\nINFLATION_COUNT_FILT_MD int64\nLongType()\nINFLATION_COUNT_FILT_MD\nINFLATION_SENT_FILT_MD object\nStringType()\nINFLATION_SENT_FILT_MD\nRESTOCKING_REL_FILT_MD float64\nFloatType()\nRESTOCKING_REL_FILT_MD\nRESTOCKING_COUNT_FILT_MD int64\nLongType()\nRESTOCKING_COUNT_FILT_MD\nRESTOCKING_SENT_FILT_MD object\nStringType()\nRESTOCKING_SENT_FILT_MD\nLIQUIDITY_REL_FILT_MD float64\nFloatType()\nLIQUIDITY_REL_FILT_MD\nLIQUIDITY_COUNT_FILT_MD int64\nLongType()\nLIQUIDITY_COUNT_FILT_MD\nLIQUIDITY_SENT_FILT_MD object\nStringType()\nLIQUIDITY_SENT_FILT_MD\nGEOPOL_UNCERTAINTY_REL_FILT_MD float64\nFloatType()\nGEOPOL_UNCERTAINTY_REL_FILT_MD\nGEOPOL_UNCERTAINTY_COUNT_FILT_MD int64\nLongType()\nGEOPOL_UNCERTAINTY_COUNT_FILT_MD\nGEOPOL_UNCERTAINTY_SENT_FILT_MD object\nStringType()\nGEOPOL_UNCERTAINTY_SENT_FILT_MD\nPROBLEM_REL_FILT_MD float64\nFloatType()\nPROBLEM_REL_FILT_MD\nPROBLEM_COUNT_FILT_MD int64\nLongType()\nPROBLEM_COUNT_FILT_MD\nPROBLEM_SENT_FILT_MD object\nStringType()\nPROBLEM_SENT_FILT_MD\nRECESSION_MELTDOWN_BLACK_SWAN_REL_FILT_MD float64\nFloatType()\nRECESSION_MELTDOWN_BLACK_SWAN_REL_FILT_MD\nRECESSION_MELTDOWN_BLACK_SWAN_COUNT_FILT_MD int64\nLongType()\nRECESSION_MELTDOWN_BLACK_SWAN_COUNT_FILT_MD\nRECESSION_MELTDOWN_BLACK_SWAN_SENT_FILT_MD object\nStringType()\nRECESSION_MELTDOWN_BLACK_SWAN_SENT_FILT_MD\nECONOMIC_DOWNTURN_REL_FILT_MD float64\nFloatType()\nECONOMIC_DOWNTURN_REL_FILT_MD\nECONOMIC_DOWNTURN_COUNT_FILT_MD int64\nLongType()\nECONOMIC_DOWNTURN_COUNT_FILT_MD\nECONOMIC_DOWNTURN_SENT_FILT_MD object\nStringType()\nECONOMIC_DOWNTURN_SENT_FILT_MD\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_REL_FILT_MD float64\nFloatType()\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_REL_FILT_MD\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_COUNT_FILT_MD int64\nLongType()\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_COUNT_FILT_MD\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_SENT_FILT_MD object\nStringType()\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_SENT_FILT_MD\nSUPPLY_CHAIN_SHORTAGE_REL_FILT_MD float64\nFloatType()\nSUPPLY_CHAIN_SHORTAGE_REL_FILT_MD\nSUPPLY_CHAIN_SHORTAGE_COUNT_FILT_MD int64\nLongType()\nSUPPLY_CHAIN_SHORTAGE_COUNT_FILT_MD\nSUPPLY_CHAIN_SHORTAGE_SENT_FILT_MD object\nStringType()\nSUPPLY_CHAIN_SHORTAGE_SENT_FILT_MD\nJOB_CREATION_REL_FILT_QA float64\nFloatType()\nJOB_CREATION_REL_FILT_QA\nJOB_CREATION_COUNT_FILT_QA int64\nLongType()\nJOB_CREATION_COUNT_FILT_QA\nJOB_CREATION_SENT_FILT_QA object\nStringType()\nJOB_CREATION_SENT_FILT_QA\nJOB_REDUCTION_REL_FILT_QA float64\nFloatType()\nJOB_REDUCTION_REL_FILT_QA\nJOB_REDUCTION_COUNT_FILT_QA int64\nLongType()\nJOB_REDUCTION_COUNT_FILT_QA\nJOB_REDUCTION_SENT_FILT_QA object\nStringType()\nJOB_REDUCTION_SENT_FILT_QA\nLABOR_SHORTAGE_REL_FILT_QA float64\nFloatType()\nLABOR_SHORTAGE_REL_FILT_QA\nLABOR_SHORTAGE_COUNT_FILT_QA int64\nLongType()\nLABOR_SHORTAGE_COUNT_FILT_QA\nLABOR_SHORTAGE_SENT_FILT_QA object\nStringType()\nLABOR_SHORTAGE_SENT_FILT_QA\nAUTOMATION_REL_FILT_QA float64\nFloatType()\nAUTOMATION_REL_FILT_QA\nAUTOMATION_COUNT_FILT_QA int64\nLongType()\nAUTOMATION_COUNT_FILT_QA\nAUTOMATION_SENT_FILT_QA object\nStringType()\nAUTOMATION_SENT_FILT_QA\nEDUCATION_TRAINING_PROGRAM_REL_FILT_QA float64\nFloatType()\nEDUCATION_TRAINING_PROGRAM_REL_FILT_QA\nEDUCATION_TRAINING_PROGRAM_COUNT_FILT_QA int64\nLongType()\nEDUCATION_TRAINING_PROGRAM_COUNT_FILT_QA\nEDUCATION_TRAINING_PROGRAM_SENT_FILT_QA object\nStringType()\nEDUCATION_TRAINING_PROGRAM_SENT_FILT_QA\nWAGE_INCREASES_REL_FILT_QA float64\nFloatType()\nWAGE_INCREASES_REL_FILT_QA\nWAGE_INCREASES_COUNT_FILT_QA int64\nLongType()\nWAGE_INCREASES_COUNT_FILT_QA\nWAGE_INCREASES_SENT_FILT_QA object\nStringType()\nWAGE_INCREASES_SENT_FILT_QA\nCHALLEGING_UNCERTAIN_VOLATILE_REL_FILT_QA float64\nFloatType()\nCHALLEGING_UNCERTAIN_VOLATILE_REL_FILT_QA\nCHALLEGING_UNCERTAIN_VOLATILE_COUNT_FILT_QA int64\nLongType()\nCHALLEGING_UNCERTAIN_VOLATILE_COUNT_FILT_QA\nCHALLEGING_UNCERTAIN_VOLATILE_SENT_FILT_QA object\nStringType()\nCHALLEGING_UNCERTAIN_VOLATILE_SENT_FILT_QA\nCOSTCUTTING_REL_FILT_QA float64\nFloatType()\nCOSTCUTTING_REL_FILT_QA\nCOSTCUTTING_COUNT_FILT_QA int64\nLongType()\nCOSTCUTTING_COUNT_FILT_QA\nCOSTCUTTING_SENT_FILT_QA object\nStringType()\nCOSTCUTTING_SENT_FILT_QA\nDISRUPTION_REL_FILT_QA float64\nFloatType()\nDISRUPTION_REL_FILT_QA\nDISRUPTION_COUNT_FILT_QA int64\nLongType()\nDISRUPTION_COUNT_FILT_QA\nDISRUPTION_SENT_FILT_QA object\nStringType()\nDISRUPTION_SENT_FILT_QA\nINFLATION_REL_FILT_QA float64\nFloatType()\nINFLATION_REL_FILT_QA\nINFLATION_COUNT_FILT_QA int64\nLongType()\nINFLATION_COUNT_FILT_QA\nINFLATION_SENT_FILT_QA object\nStringType()\nINFLATION_SENT_FILT_QA\nRESTOCKING_REL_FILT_QA float64\nFloatType()\nRESTOCKING_REL_FILT_QA\nRESTOCKING_COUNT_FILT_QA int64\nLongType()\nRESTOCKING_COUNT_FILT_QA\nRESTOCKING_SENT_FILT_QA object\nStringType()\nRESTOCKING_SENT_FILT_QA\nLIQUIDITY_REL_FILT_QA float64\nFloatType()\nLIQUIDITY_REL_FILT_QA\nLIQUIDITY_COUNT_FILT_QA int64\nLongType()\nLIQUIDITY_COUNT_FILT_QA\nLIQUIDITY_SENT_FILT_QA object\nStringType()\nLIQUIDITY_SENT_FILT_QA\nGEOPOL_UNCERTAINTY_REL_FILT_QA float64\nFloatType()\nGEOPOL_UNCERTAINTY_REL_FILT_QA\nGEOPOL_UNCERTAINTY_COUNT_FILT_QA int64\nLongType()\nGEOPOL_UNCERTAINTY_COUNT_FILT_QA\nGEOPOL_UNCERTAINTY_SENT_FILT_QA object\nStringType()\nGEOPOL_UNCERTAINTY_SENT_FILT_QA\nPROBLEM_REL_FILT_QA float64\nFloatType()\nPROBLEM_REL_FILT_QA\nPROBLEM_COUNT_FILT_QA int64\nLongType()\nPROBLEM_COUNT_FILT_QA\nPROBLEM_SENT_FILT_QA object\nStringType()\nPROBLEM_SENT_FILT_QA\nRECESSION_MELTDOWN_BLACK_SWAN_REL_FILT_QA float64\nFloatType()\nRECESSION_MELTDOWN_BLACK_SWAN_REL_FILT_QA\nRECESSION_MELTDOWN_BLACK_SWAN_COUNT_FILT_QA int64\nLongType()\nRECESSION_MELTDOWN_BLACK_SWAN_COUNT_FILT_QA\nRECESSION_MELTDOWN_BLACK_SWAN_SENT_FILT_QA object\nStringType()\nRECESSION_MELTDOWN_BLACK_SWAN_SENT_FILT_QA\nECONOMIC_DOWNTURN_REL_FILT_QA float64\nFloatType()\nECONOMIC_DOWNTURN_REL_FILT_QA\nECONOMIC_DOWNTURN_COUNT_FILT_QA int64\nLongType()\nECONOMIC_DOWNTURN_COUNT_FILT_QA\nECONOMIC_DOWNTURN_SENT_FILT_QA object\nStringType()\nECONOMIC_DOWNTURN_SENT_FILT_QA\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_REL_FILT_QA float64\nFloatType()\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_REL_FILT_QA\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_COUNT_FILT_QA int64\nLongType()\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_COUNT_FILT_QA\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_SENT_FILT_QA object\nStringType()\nBOTTLENECK_SHORTAGE_LOGISTIC_CONSTRAINT_SENT_FILT_QA\nSUPPLY_CHAIN_SHORTAGE_REL_FILT_QA float64\nFloatType()\nSUPPLY_CHAIN_SHORTAGE_REL_FILT_QA\nSUPPLY_CHAIN_SHORTAGE_COUNT_FILT_QA int64\nLongType()\nSUPPLY_CHAIN_SHORTAGE_COUNT_FILT_QA\nSUPPLY_CHAIN_SHORTAGE_SENT_FILT_QA object\nStringType()\nSUPPLY_CHAIN_SHORTAGE_SENT_FILT_QA\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:401: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n  Expected a string or bytes dtype, got datetime64[us]\nAttempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "for col in ['SENT_LABELS_FILT_MD','SENT_LABELS_FILT_QA']:\n",
    "  currdf_old[col] = currdf_old[col].apply(ast.literal_eval).astype(object)\n",
    "\n",
    "\n",
    "spark_parsedDF_old = pandas_to_spark_old(currdf_old)\n",
    "spark_parsedDF_old = spark_parsedDF_old.replace(np.nan, None)\n",
    "spark_parsedDF_old = spark_parsedDF_old.withColumn(\"DATE\", F.to_timestamp(spark_parsedDF_old.DATE, 'yyyy-MM-dd'))\n",
    "spark_parsedDF_old = spark_parsedDF_old.withColumn(\"PARSED_DATETIME_EASTERN_TZ\", F.to_timestamp(spark_parsedDF_old.PARSED_DATETIME_EASTERN_TZ, 'yyyy-MM-dd HH mm ss'))\n",
    "spark_parsedDF_old = spark_parsedDF_old.withColumn(\"EVENT_DATETIME_UTC\", F.to_timestamp(spark_parsedDF_old.EVENT_DATETIME_UTC, 'yyyy-MM-dd HH mm ss'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c7869f-7a8d-48ea-99d5-a98ebcd4f603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efec933d-5d7f-4f98-bc6a-319276413c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_parsedDF_ref = pandas_to_spark(currdf_ref, column_type_mapping = {'FILT_MD' :  ArrayType(StringType()),\n",
    "                                                                'FILT_QA' :  ArrayType(StringType()),\n",
    "                                                                '_len_' :  ArrayType(IntegerType()),\n",
    "                                                                '_total_' :  ArrayType(IntegerType()),\n",
    "                                                                '_count_' :  IntegerType(),\n",
    "                                                                '_stats_' :  MapType(StringType(), IntegerType()),\n",
    "                                                                'sent_scores' :  ArrayType(FloatType()),\n",
    "                                                                'sent_labels' :  ArrayType(IntegerType())}, spark = spark)\n",
    "spark_parsedDF_ref = spark_parsedDF_ref.replace(np.nan, None)\n",
    "\n",
    "convert_columns_to_timestamp(spark_parsedDF_ref, columns_formats = {'DATE': 'yyyy-MM-dd',\n",
    "                                                                    'PARSED_DATETIME_EASTERN_TZ': 'yyyy-MM-dd HH mm ss',\n",
    "                                                                    'EVENT_DATETIME_UTC': 'yyyy-MM-dd HH mm ss'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b656668-41bf-4991-a526-53bb94ce4ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_parsedDF_old = spark_parsedDF_old.toPandas()\n",
    "spark_parsedDF_ref = spark_parsedDF_ref.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c45319-0466-4d1a-b01e-79b447c42660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert spark_parsedDF_ref.equals(spark_parsedDF_old), \"The DataFrames are not identical.\"\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Dictionary_based_method_demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}