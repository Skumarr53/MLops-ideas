Thanks for the clarification. I’ll investigate why the dictionary-based input format in Method 2 might be causing higher memory usage with Hugging Face's `pipeline`, especially in a Spark distributed setting. I’ll explore how `nli_pipeline` internally handles input tokenization for both formats and whether one results in a significantly larger memory footprint.

I’ll get back with insights and possible workarounds—including optimizations for Spark batch processing and tokenizer behavior.

# Understanding Dictionary vs. String Inputs in Hugging Face NLI Pipeline

## Pipeline Tokenization for `text` vs. `text_pair` Inputs

Hugging Face’s `transformers.pipeline` can accept either a single string or a dictionary with `"text"` and `"text_pair"` for pairwise inputs ([transformers/src/transformers/pipelines/text_classification.py at main · huggingface/transformers · GitHub](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/text_classification.py#:~:text=inputs%20%28%60str%60%20or%20%60List,str)). Under the hood, both forms ultimately produce the same token sequence for premise-hypothesis pairs, but the process differs:

- **Dictionary input (`{"text": premise, "text_pair": hypothesis}`):** The pipeline recognizes two separate sequences. It calls the tokenizer with the `text` and `text_pair` arguments, which causes the tokenizer to **individually encode** the premise and hypothesis before concatenation with special tokens ([transformers/src/transformers/pipelines/text_classification.py at main · huggingface/transformers · GitHub](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/text_classification.py#:~:text=if%20isinstance)) ([transformers/src/transformers/pipelines/text_classification.py at main · huggingface/transformers · GitHub](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/text_classification.py#:~:text=return%20self)). For example, BERT-based models will insert `[CLS]` at the beginning, then encode the premise, add `[SEP]`, encode the hypothesis, and end with another `[SEP]`. Models like RoBERTa/BART (which use `<s>` and `</s>` tokens) will similarly add a separator token between sequences (often appearing as `</s></s>` in text form) and appropriate end-of-sequence tokens. This approach clearly delineates the two inputs for the model. 

- **Single string input (`"premise</s></s>hypothesis"` or `"premise [SEP] hypothesis"`):** Here, the pipeline treats the entire string as one sequence. The user-manually inserted separator tokens (e.g. the literal `</s></s>` for BART/RoBERTa or `[SEP]` for BERT) become part of the text fed to the tokenizer. The tokenizer then processes this **as one combined text**. In practice, for models that rely solely on special tokens (and not segment embeddings) this yields the same token IDs as the two-sequence method – e.g. for RoBERTa the tokenized result of `"premise</s></s>hypothesis"` will include the same `<s>` (BOS) at start and `</s>` separators as the dictionary method would. The key difference is that the tokenizer sees one continuous input, not two separate fields.

## Memory Implications of Dictionary Inputs 

Using a dictionary for NLI pairs can consume more memory than a single concatenated string, especially in a batched setting. There are a few reasons for this:

- **Multiple intermediate objects:** When using the `{"text", "text_pair"}` format with a batch of inputs, the pipeline will tokenize each pair (premise, hypothesis) separately (internally looping or handling each item) before batching. This means it temporarily holds many small `BatchEncoding` or tensor objects – one per example – in memory. Each example may include its own copy of arrays like `input_ids`, `attention_mask`, and potentially `token_type_ids` (for models that use segment embeddings). For 64 inputs, that’s 64 separate tokenization results in memory before they are collated. In contrast, when providing a list of concatenated strings, the tokenizer can often handle the entire batch in one call (producing one large `BatchEncoding` for all 64), reducing overhead. In older versions of Transformers, passing a list of strings was handled in one vectorized call to the fast tokenizer ([Memory leak for large strings · Issue #1539 · huggingface/tokenizers · GitHub](https://github.com/huggingface/tokenizers/issues/1539#:~:text=Notes)), whereas list of dicts was not directly vectorized. This can lead to **higher peak memory usage** with dictionary inputs due to the proliferation of Python objects and small tensors.

- **Tokenization caching and overhead:** The fast tokenizer library may not efficiently reuse work across many separate calls. When 64 pairs are tokenized individually, there is repeated overhead (e.g. repeatedly allocating buffers, parsing text) for each call. GitHub issue reports have shown that tokenizing many *unique* strings sequentially can lead to memory spikes and slower processing ([Memory leak for large strings · Issue #1539 · huggingface/tokenizers · GitHub](https://github.com/huggingface/tokenizers/issues/1539#:~:text=This%20is%20rather%20severe%2C%20not,is%20also%20much%2C%20much%20lower)). In one analysis, memory usage grew substantially as unique strings were tokenized one by one, whereas processing them in one batch (or having repetitive substrings/spaces) was much more memory-efficient ([Memory leak for large strings · Issue #1539 · huggingface/tokenizers · GitHub](https://github.com/huggingface/tokenizers/issues/1539#:~:text=Notes)). Using dictionary inputs in the pipeline may trigger a similar pattern – each premise/hypothesis pair is an “unique” call to the tokenizer, preventing potential optimizations. By contrast, a single concatenated string per example might contain repeated separator tokens and allow the tokenizer to handle the batch in one go, avoiding some overhead.

- **Additional fields and padding:** With the dictionary method, the tokenizer knows it is dealing with two sequences. For models like BERT/DistilBERT, it will generate a `token_type_ids` tensor to distinguish the segments (0 for premise tokens, 1 for hypothesis). This is an extra tensor of the same length as the input, effectively doubling the token-id related memory for those models. (If using RoBERTa/BART, `token_type_ids` are not used, so this might not apply.) If passing a single string, the tokenizer might not generate `token_type_ids` at all (for RoBERTa/BART it wouldn’t, and for BERT it would generate all 0s if it assumes a single sequence). Moreover, in a **batch of mixed-length pairs**, the pipeline will pad all examples to the length of the longest sequence ([Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation#:~:text=The%20,a%20boolean%20or%20a%20string)) ([Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation#:~:text=The%20,a%20boolean%20or%20a%20string)). If one or two examples are significantly longer, using the dictionary approach means both sequences are kept to full length before truncation/padding, possibly increasing the max length. The concatenated string approach is functionally the same in terms of padding, but any inefficiency in truncation strategy (e.g. default `longest_first` might trim some from both sequences) would apply equally. The main difference is that with dict inputs the pipeline by default might allow very long combined sequences unless you enable truncation, which can blow up memory.

**Bottom line:** The dictionary method is straightforward and recommended for correctness ([transformers/src/transformers/pipelines/text_classification.py at main · huggingface/transformers · GitHub](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/text_classification.py#:~:text=inputs%20%28%60str%60%20or%20%60List,str)), but it can create more transient memory load. Each input pair is handled as a separate entry, leading to more memory allocation **during tokenization and collation**. The single-string hack forces the work into one sequence per example, potentially making tokenization internally simpler and lighter on memory (at the cost of some clarity). In a PySpark distributed scenario, where multiple processes handle batches of size 64, the higher memory usage with dict inputs can be significant – it can contribute to executor memory pressure or even OOM if many processes each hold large tokenization outputs in memory.

## Batch Processing Differences (`List[dict]` vs `List[str]`)

When you provide a batch of inputs, the pipeline will try to process them in a batch for efficiency. However, the treatment of `List[dict]` vs `List[str]` has subtle differences:

- With a list of dictionaries, the **pipeline expects text pairs** and will not directly pass the list to the tokenizer in one go (indeed, the `TextClassificationPipeline` raises an error if you incorrectly pass a list of lists instead of a dict for pairs) ([transformers/src/transformers/pipelines/text_classification.py at main · huggingface/transformers · GitHub](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/text_classification.py#:~:text=elif%20isinstance)). Instead, it likely iterates internally. In current versions, the pipeline does batch-collation after tokenizing each pair, meaning it will pad and stack those 64 encodings into one big batch tensor for the model forward pass. The model *is* ultimately fed a single batched tensor of shape (64, max_seq_len). So the **model inference** uses the same amount of GPU/CPU memory either way. The **difference lies in the tokenizer stage** – with dicts, it does 64 smaller tokenization operations then merges, whereas with 64 raw strings, the tokenizer can often handle it as one vectorized batch. In other words, `tokenizer(batch_texts, batch_text_pairs)` is more memory-efficient than calling `tokenizer(text, text_pair)` 64 times in a loop. The pipeline’s design doesn’t currently merge `batch_text_pairs` automatically (it requires the dict format for pairs), so it ends up doing the loop internally. This is why you observed a big memory difference.

- With a list of strings (already concatenated), the pipeline essentially sees 64 single-sequence inputs. It can tokenize them in one call (fast tokenizers accept a list of strings and return a batch) or at least in a more optimized manner. There is no need to manage two separate sequences per item. This **reduces Python-level overhead** and can reuse internal buffers. Empirically, users have found that using a single fused string avoids the memory spikes seen when processing many unique pairs individually ([Memory leak for large strings · Issue #1539 · huggingface/tokenizers · GitHub](https://github.com/huggingface/tokenizers/issues/1539#:~:text=Notes)). In short, **List[str] input can be tokenized as a batch**, whereas **List[dict] input results in a batch of tokenizations**.

Lastly, consider that some models (e.g. BART-based zero-shot classifiers) return decoder caches (`past_key_values`) by default which can blow up memory if not disabled. The Transformers pipeline explicitly sets `use_cache=False` for sequence classification models to prevent this kind of accumulation ([transformers/src/transformers/pipelines/text_classification.py at main · huggingface/transformers · GitHub](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/text_classification.py#:~:text=def%20_forward)). Thus, the higher memory usage with dict inputs is not due to caching in the model, but rather due to preprocessing.

## Strategies to Reduce Memory Usage for NLI Inference

To mitigate the memory overhead of dictionary-style inputs in a distributed batch setting, consider the following recommendations:

- **Split into Smaller Batches:** Instead of processing 64 pairs at once, try splitting the workload into smaller batches (e.g. 16 or 32). This reduces peak memory usage at the cost of a bit more runtime overhead. You can do this by adjusting the pipeline’s `batch_size` parameter or manually chunking your data. For example, process 64 inputs as two sequential batches of 32. This way, the tokenizer and model only ever handle at most 32 at a time, halving the transient memory required for tokenization and padding. In a PySpark scenario, you might ensure that the RDD/DataFrame is partitioned accordingly or use `.mapPartitions` to control batch sizes. The Transformers documentation notes that padding to the longest sequence in a batch is the default ([Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation#:~:text=The%20,a%20boolean%20or%20a%20string)) – by keeping batches smaller, you also reduce the chance that a single extremely long pair forces a lot of padding on all 64 examples. In summary, **smaller batches = less peak memory**.

- **Enable Truncation with an Appropriate Strategy:** If your premises or hypotheses can be very long, it’s crucial to enable `truncation=True` with a max length. Even better, choose a truncation strategy that prioritizes one of the two sequences. For NLI, usually the premise is important to keep in full, and you might truncate the hypothesis if needed. You can achieve this by `pipeline(..., truncation="only_second", max_length=N)`. The `"only_second"` option will *only cut tokens from the second sequence (hypothesis) if the pair is too long* ([Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation#:~:text=,This%20is%20the)). This ensures the premise isn’t inadvertently chopped off. Using truncation prevents pathological memory use when a pair exceeds the model’s typical input length (e.g. BERT models usually max out at 512 tokens). Without truncation, a long text pair could produce a very large tensor, especially when batched, consuming a lot of memory. By truncating to, say, 256 or 512 tokens total, you put an upper bound on memory per example. This is an **effective safeguard** in production pipelines.

- **Use Alternative Input Formatting (with Caution):** As you discovered, passing a single concatenated string with a separator can drastically cut memory overhead during tokenization. For models that **do not use token type IDs** (RoBERTa, DistilRoBERTa, XLNet, BART, etc.), this trick is usually safe – the model sees the special separator token and internally it’s equivalent to having used `text_pair`. For example, the BART-large MNLI model expects input like `<s> premise </s> </s> hypothesis </s>`; whether you provide that via the official two-field method or as one string containing `</s></s>` doesn’t change the encoded result. By using one string, you force the tokenizer to do one pass per example, which can save memory. **However, use caution with BERT or other models that rely on segment embeddings**. If you feed BERT a single string `"premise [SEP] hypothesis"`, the tokenizer will by default treat it all as segment 0 (because it doesn’t know a second segment was intended). The model will not receive segment IDs to distinguish premise vs hypothesis, which *could* impact predictions (BERT was trained with token_type_ids for NLI). In practice, the presence of an actual `[SEP]` token in the sequence might allow the model to infer the separation, but it’s not guaranteed to perform exactly the same as the two-sequence method. So, **if you use the concatenation trick, prefer it for models where `type_vocab_size=1` or segment IDs are unused** (like Roberta or Bart). Always test that the predictions don’t change significantly. This method isn’t officially documented by Hugging Face (it’s more of a hack), so ensure it fits your use case.

- **Pre-tokenize or use the Tokenizer directly:** For more control, you can bypass the pipeline’s automated preprocessing. For example, use `AutoTokenizer` to encode your batch of premises and hypotheses in a memory-efficient way (e.g., by calling `tokenizer(premises, hypotheses, padding=True, truncation=True, return_tensors='pt')` directly). This leverages the tokenizer’s optimized batch processing for pairs. Then you can feed the resulting tensors into `AutoModelForSequenceClassification` manually. This avoids the pipeline’s internal loop and gives you the opportunity to free intermediate results immediately. It also makes it easier to implement custom batch sizing or truncation logic. The downside is you manage more code yourself, but it can be worth it in a Spark setting where you want to minimize memory overhead per worker. (If you stick with the `pipeline` object, you could still achieve something similar by batching outside of it.)

- **General memory-saving tips:** Make sure you’re using the latest Transformers version, as improvements have been made to memory handling (for instance, as of v4.20+, the pipeline disables decoder caches and you can load models with `low_cpu_mem_usage=True` to avoid double-loading the weights in memory during initialization ([Double expected memory usage - Beginners - Hugging Face Forums](https://discuss.huggingface.co/t/double-expected-memory-usage/15190#:~:text=So%20the%20bad%20news%20it,the%20RAM%20of%20the%20model))). If running on CPU, you won’t have GPU memory issues, but you might benefit from using FP16 models on GPU to halve memory usage. In a distributed scenario, if each executor holds a copy of the model, ensure the model is as small as feasible (e.g., use DistilBERT or MiniLM variants for NLI instead of a huge BART, if latency/accuracy trade-off permits). A smaller model means less memory footprint for both model and tokenized inputs.

By applying these strategies, you should see a reduction in memory consumption. For example, using truncation to limit sequence length and splitting the 64 pairs into two batches of 32 could dramatically cut peak RAM usage, with only a minor hit to throughput. In summary, **tokenize smarter and batch smaller**. This aligns the pipeline’s behavior closer to the single-string method (or even improves on it) while maintaining the officially supported usage of `text_pair` inputs. Always monitor memory when scaling out in PySpark, as each worker handling a large batch can amplify usage across the cluster.

## References

- Hugging Face Transformers pipeline documentation on providing text pairs (premise/hypothesis) ([transformers/src/transformers/pipelines/text_classification.py at main · huggingface/transformers · GitHub](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/text_classification.py#:~:text=inputs%20%28%60str%60%20or%20%60List,str)). This shows that the recommended way to supply pairs is via a dictionary with `"text"` and `"text_pair"` keys.  
- Hugging Face Transformers code (TextClassificationPipeline) illustrating how dictionary vs. list inputs are handled ([transformers/src/transformers/pipelines/text_classification.py at main · huggingface/transformers · GitHub](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/text_classification.py#:~:text=if%20isinstance)) ([transformers/src/transformers/pipelines/text_classification.py at main · huggingface/transformers · GitHub](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/text_classification.py#:~:text=elif%20isinstance)). The pipeline tokenizes dict inputs by calling the tokenizer with separate `text` and `text_pair`, which can incur per-item processing.  
- Discussion in Hugging Face tokenizers library about memory usage per string ([Memory leak for large strings · Issue #1539 · huggingface/tokenizers · GitHub](https://github.com/huggingface/tokenizers/issues/1539#:~:text=Notes)). It was observed that tokenizing many unique strings sequentially caused excessive memory growth, whereas having common substrings or doing it in one batch was more memory-efficient. This supports the notion that one batched call (as in the single-string approach) uses memory more sparingly than many individual calls.  
- Hugging Face documentation on truncation strategies ([Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation#:~:text=,This%20is%20the)). It defines the `"only_second"` truncation mode, which is useful for preserving the first sequence (premise) and truncating the second (hypothesis) to control input size.  
- **Hugging Face Forums –** users noting high memory when loading or using large models and solutions like `low_cpu_mem_usage=True` ([Double expected memory usage - Beginners - Hugging Face Forums](https://discuss.huggingface.co/t/double-expected-memory-usage/15190#:~:text=So%20the%20bad%20news%20it,the%20RAM%20of%20the%20model)) (while not directly about tokenization, it’s a general tip to avoid doubling memory use on model load, which can help in Spark executors).