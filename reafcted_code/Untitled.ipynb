{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9192c3b-b720-4911-9342-731b0c529d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, size, expr\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType, IntegerType, StructType, StructField, TimestampType\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1b5887-853a-467b-ba43-170b686cdd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "GPU Device Name: NVIDIA GeForce GTX 1660\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40a89bd4-bd77-45be-a710-69fd49a3ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86c1da69-c080-43ad-859a-58a6c4d49ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Constants (Ensure these are correctly defined)\n",
    "MODEL_FOLDER_PATH = \"~/.cache/huggingface/hub/\"  # Update this path\n",
    "MODEL_NAME = \"models--MoritzLaurer--deberta-v3-large-zeroshot-v2.0/\"\n",
    "ENABLE_QUANTIZATION = False\n",
    "BATCH_SIZE = 1  # Adjust based on your GPU capacity\n",
    "\n",
    "# Step 2: Initialize Logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(\"NLI_Inference\")\n",
    "\n",
    "# Initialize a global variable for the pipeline\n",
    "nli_pipeline = None\n",
    "\n",
    "def initialize_nli_pipeline(enable_quantization=False):\n",
    "    global nli_pipeline\n",
    "    try:\n",
    "        # Load the tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Load the model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        \n",
    "        if enable_quantization:\n",
    "            model = torch.quantization.quantize_dynamic(\n",
    "                model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "            )\n",
    "            logger.info(\"Model quantization enabled.\")\n",
    "        else:\n",
    "            logger.info(\"Model quantization disabled.\")\n",
    "        \n",
    "        device = -1 if torch.cuda.is_available() else -1\n",
    "        nli_pipeline = pipeline(\n",
    "            task=\"zero-shot-classification\",  # Changed task\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(f\"NLI pipeline initialized on device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize NLI pipeline: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Define the schema for the inference results\n",
    "inference_schema = ArrayType(StructType([\n",
    "    StructField(\"label\", StringType(), False),\n",
    "    StructField(\"score\", FloatType(), False)\n",
    "]))\n",
    "\n",
    "@pandas_udf(inference_schema, PandasUDFType.SCALAR_ITER)\n",
    "def md_inference_udf(iterator):\n",
    "    global nli_pipeline\n",
    "    if nli_pipeline is None:\n",
    "        initialize_nli_pipeline(enable_quantization=ENABLE_QUANTIZATION)\n",
    "    \n",
    "    for batch_num, batch in enumerate(iterator, start=1):\n",
    "        logger.info(f\"Processing MD inference batch {batch_num} with {len(batch)} rows.\")\n",
    "        try:\n",
    "            # Each 'batch' is a Pandas Series containing lists of text pairs\n",
    "            # Flatten the list of text pairs in the batch\n",
    "            flat_text_pairs = [pair for sublist in batch for pair in sublist]\n",
    "            logger.debug(f\"Batch {batch_num}: Total text pairs to infer: {len(flat_text_pairs)}\")\n",
    "            \n",
    "            if len(flat_text_pairs):\n",
    "                # Perform inference in batch using zero-shot-classification pipeline\n",
    "                results = nli_pipeline(\n",
    "                    sequences=flat_text_pairs,\n",
    "                    candidate_labels=[\n",
    "                        \"This text is about consumer strength\",\n",
    "                        \"This text is about consumer weakness\",\n",
    "                        \"This text is about reduced consumer's spending patterns\"\n",
    "                    ],\n",
    "                    multi_class=True,\n",
    "                    batch_size=BATCH_SIZE\n",
    "                )\n",
    "                logger.debug(f\"Batch {batch_num}: Inference completed with {len(results)} results.\")\n",
    "            else:\n",
    "                results = []\n",
    "            \n",
    "            # Split results back to original rows\n",
    "            split_results = []\n",
    "            idx = 0\n",
    "            for pairs in batch:\n",
    "                if len(pairs):\n",
    "                    # Extract the relevant slice of results\n",
    "                    row_results = results[idx:idx+len(pairs)]\n",
    "                    # Transform each result into a list of dicts with 'label' and 'score'\n",
    "                    formatted_results = [\n",
    "                        {\"label\": res['labels'][i], \"score\": res['scores'][i]}\n",
    "                        for res in row_results\n",
    "                        for i in range(len(res['labels']))\n",
    "                    ]\n",
    "                    split_results.append(formatted_results)\n",
    "                    idx += len(pairs)\n",
    "                else:\n",
    "                    split_results.append([])\n",
    "            \n",
    "            yield pd.Series(split_results)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in MD inference batch {batch_num}: {e}\")\n",
    "            # Yield empty results for this batch to continue processing\n",
    "            yield pd.Series([[] for _ in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d62080c-6e51-4202-8a57-12318b4e95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"TEXT_PAIRS_MD\": [\n",
    "            [\"Consumers are showing increased confidence in the market, leading to higher spending.<s><s>This text is about consumer strength\"],\n",
    "            [\"There is a noticeable decline in consumer purchasing behavior this quarter.<s><s>This text is about consumer weakness\"]\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5993530e-310d-435c-b689-d87ba15b5365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |TEXT_PAIRS_MD                                                                                                                                                                                                                                               |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |[[Consumers are showing increased confidence in the market, leading to higher spending.<s><s>This text is about consumer strength], [There is a noticeable decline in consumer purchasing behavior this quarter.<s><s>This text is about consumer weakness]]|\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NLI_Inference_Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"TEXT_PAIRS_MD\", ArrayType(ArrayType(StringType())), False)  # Array of arrays of strings\n",
    "])\n",
    "\n",
    "# Create the Spark DataFrame\n",
    "sample_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "sample_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "943426a1-4f3b-4265-8de7-690f9c43175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `multi_class` argument has been deprecated and renamed to `multi_label`. `multi_class` will be removed in a future version of Transformers.\n",
      "/home/skumar/miniconda3/envs/spark/lib/python3.8/site-packages/torch/utils/data/dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x70c9192d7f70> was reported to be 2(when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/skumar/miniconda3/envs/spark/lib/python3.8/site-packages/torch/utils/data/dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x70c9192d7f70> was reported to be 2(when accessing len(dataloader)), but 4 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/skumar/miniconda3/envs/spark/lib/python3.8/site-packages/torch/utils/data/dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x70c9192d7f70> was reported to be 2(when accessing len(dataloader)), but 5 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/skumar/miniconda3/envs/spark/lib/python3.8/site-packages/torch/utils/data/dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x70c9192d7f70> was reported to be 2(when accessing len(dataloader)), but 6 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |MD_RESULT                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |[{This text is about consumer strength, 0.9993011}, {This text is about reduced consumer's spending patterns, 1.02624545E-4}, {This text is about consumer weakness, 9.501615E-5}, {This text is about consumer weakness, 0.999502}, {This text is about reduced consumer's spending patterns, 0.9985817}, {This text is about consumer strength, 3.758918E-4}]|\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Apply the UDF\n",
    "result_df = sample_df.withColumn('MD_RESULT', md_inference_udf(col('TEXT_PAIRS_MD')))\n",
    "\n",
    "# Display the results\n",
    "result_df.select(\"id\", \"MD_RESULT\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2430e-d821-48e2-935a-9cd227a02670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e95da2-7016-4747-bb65-b026f622f14f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
